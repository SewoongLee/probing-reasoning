{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sample a smaller dataset from gsm\n",
    "\n",
    "# import json\n",
    "# import random\n",
    "\n",
    "# # Set the seed for reproducibility\n",
    "# random.seed(42)\n",
    "\n",
    "# # Input and output file paths\n",
    "# input_file = 'train-gsm.jsonl'\n",
    "# output_file = 'train-gsm-300.jsonl'\n",
    "\n",
    "# # Number of lines to sample\n",
    "# sample_size = 300\n",
    "\n",
    "# # Read all lines from the input file\n",
    "# with open(input_file, 'r') as f:\n",
    "#     lines = f.readlines()\n",
    "\n",
    "# # Randomly sample 250 lines\n",
    "# sampled_lines = random.sample(lines, sample_size)\n",
    "\n",
    "# # Write the sampled lines to the output file\n",
    "# with open(output_file, 'w') as f:\n",
    "#     f.writelines(sampled_lines)\n",
    "\n",
    "# print(f\"Successfully sampled {sample_size} lines and saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = \"\"\"\n",
    "I will give you a mathematical question and answer. You need to arrive at the final answer step-by-step. You need to produce 2 answers, one with valid and one with invalid mathematical reasoning. I have given you a few examples of valid and invalid mathematical reasoning in the following text. Please produce your answers as a python dictionary, (just the dictionary in plain text) with 3 fields, \"question\", \"valid reasoning answer\", and \"invalid reasoning answer\" \n",
    "\n",
    "Question: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
    "Valid reasoning answer: There are 15 trees originally. Then there were 21 trees after the Grove workers planted some more. So there must have been 21 - 15 = 6 trees that were planted. The answer is 6.\n",
    "Invalid reasoning answer: There are 15 trees originally. Then there were 21 trees after the Grove workers planted some more. Now 15 + 21 = 36. Since there were 6 workers in the grove, so the grove workers planted 36 / 6 = 6 trees today. The answer is 6.\n",
    "\n",
    "Question: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "Valid reasoning answer: There are originally 3 cars. Then 2 more cars arrive. Now 3 + 2 = 5 cars are in the parking lot. The answer is 5.\n",
    "Invalid reasoning answer: There are originally 3 cars. Then 2 more cars arrive. Now 3 * 2 = 6 cars come. So 6 - 1 = 5 cars are in the parking lot. The answer is 5.\n",
    "\n",
    "Question: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
    "Valid reasoning answer: Originally, Leah had 32 chocolates and her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39 pieces left in total. The answer is 39.\n",
    "Invalid reasoning answer: Originally, Leah had 32 chocolates and her sister had 42. So her sister had 42 - 32 = 10 chocolates more than Leah has. After eating 35, since 10 + 35 = 45, they had 45 - 6 = 39 pieces left in total. The answer is 39.\n",
    "\n",
    "Here is the question and answer for which you need to generate outputs:\n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "def generate_reasoning(question):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt_template.format(question)}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        # print(str(response.choices[0].message.content)[10:-3])\n",
    "        only_ans = question['answer'].split(\"####\")[1].strip()\n",
    "        gpt_response_dict = json.loads(response.choices[0].message.content[10:-3])\n",
    "        gpt_response_dict['only_ans'] = only_ans\n",
    "        return gpt_response_dict\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error decoding JSON for question: {question}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating reasoning for question: {question}\")\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = 'train-gsm-300.jsonl'\n",
    "output_file = 'dataset1.jsonl'\n",
    "\n",
    "# Process the input file and generate reasoning\n",
    "with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "    for i, line in tqdm(enumerate(infile)):\n",
    "        question = json.loads(line.strip())\n",
    "        result = generate_reasoning(question)\n",
    "        if result:\n",
    "            json.dump(result, outfile)\n",
    "            outfile.write('\\n')\n",
    "        time.sleep(1)  # To avoid hitting rate limits\n",
    "        # break\n",
    "\n",
    "print(f\"Processing complete. Output saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvpr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
