{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import set_seed\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sewoo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:777: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be24c8ddc504ded8d05aeec7a7757c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s],))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 3072)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
      ") LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "# model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "# model_name = \"meta-llama/Llama-3.2-3B\"\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "# model_name = \"google/gemma-2-2b\"\n",
    "# model_name = \"google/gemma-2-2b-it\"\n",
    "# model_name = \"google/gemma-2-9b\"\n",
    "# model_name = \"google/gemma-2-9b-it\"\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    output_hidden_states=True,  # Enable hidden states\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "print(model, model.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7473, 1319)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.util.json_io import *\n",
    "\n",
    "train_qnas = load_jsonlines(f'data/gsm8k/train.jsonl')\n",
    "test_qnas = load_jsonlines(f'data/gsm8k/test.jsonl')\n",
    "len(train_qnas), len(test_qnas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: For every 12 cans you recycle, you receive $0.50, and for every 5 kilograms of newspapers, you receive $1.50. If your family collected 144 cans and 20 kilograms of newspapers, how much money would you receive?\n",
      "Answer: There are 144/12 = <<144/12=12>>12 sets of 12 cans that the family collected.\n",
      "So, the family would receive $0.50 x 12 = $<<0.50*12=6>>6 for the cans.\n",
      "There are 20/5 = <<20/5=4>>4 sets of 5 kilograms of newspapers that the family collected.\n",
      "So, the family would receive $1.50 x 4 = $<<1.50*4=6>>6 for the newspapers.\n",
      "Therefore, the family would receive a total of $6 + $6 = $<<6+6=12>>12.\n",
      "#### 12\n",
      "\n",
      "Question: Betty picked 16 strawberries. Matthew picked 20 more strawberries than Betty and twice as many as Natalie. They used their strawberries to make jam. One jar of jam used 7 strawberries and they sold each jar at $4. How much money were they able to make from the strawberries they picked?\n",
      "Answer: Matthew picked 16 + 20 = <<16+20=36>>36 strawberries.\n",
      "Natalie picked 36/2 = <<36/2=18>>18 strawberries.\n",
      "All together, they have 16 + 36 + 18 = <<16+36+18=70>>70 strawberries.\n",
      "They can make 70/7 = <<70/7=10>>10 jars of strawberries.\n",
      "They earn 10 x $4 = $<<10*4=40>>40 from the strawberries they picked.\n",
      "#### 40\n",
      "\n",
      "Question: Jack has a stack of books that is 12 inches thick. He knows from experience that 80 pages is one inch thick. If he has 6 books, how many pages is each one on average?\n",
      "Answer: There are 960 pages because 80 x 12 = <<80*12=960>>960\n",
      "Each book is 160 pages because 960 / 6 = <<960/6=160>>160\n",
      "#### 160\n",
      "\n",
      "Question: James dumps his whole collection of 500 Legos on the floor and starts building a castle out of them.  He uses half the pieces before finishing and is told to put the rest away.  He puts all of the leftover pieces back in the box they came from, except for 5 missing pieces that he can't find.  How many Legos are in the box at the end?\n",
      "Answer: James starts with 500 Legos and uses half of them, leaving 500/2=<<500/2=250>>250 Legos unused.\n",
      "He puts those unused Legos away but since he's missing 5 he only puts 250-5=<<250-5=245>>245 Legos away.\n",
      "#### 245\n",
      "\n",
      "Question: Ines had $20 in her purse. She bought 3 pounds of peaches, which are $2 per pound at the local farmers’ market. How much did she have left?\n",
      "Answer: Ines bought 3 pounds of peaches for 3 peaches * $2/peach = $<<3*2=6>>6.\n",
      "Ines has $20 - $6 = $<<20-6=14>>14 left.\n",
      "#### 14\n",
      "\n",
      "Question: Aaron pays his actuary membership fees each year. The membership fee increases yearly by $10. If he pays $80 in the first year, how much does his membership cost, in dollars, in the sixth year?\n",
      "Answer: In year 2 he pays 80+10=$<<80+10=90>>90.\n",
      "In year 3 he pays 90+10=$<<90+10=100>>100.\n",
      "In year 4 he pays 100+10=$<<100+10=110>>110.\n",
      "In year 5 he pays 110+10=$<<110+10=120>>120.\n",
      "In year 6 he pays 120+10=$<<120+10=130>>130.\n",
      "#### 130\n",
      "\n",
      "Question: Joseph invested $1000 into a hedge fund. The fund promised a yearly interest rate of 10%. If he deposited an additional $100 every month into the account to add to his initial investment of $1000, how much money will he have in the fund after two years?\n",
      "Answer: For the first year, Joseph will have invested $1000 + ($100 * 12) = $<<1000+100*12=2200>>2200.\n",
      "The interest calculated for the first year will be $2200 * 10% = $<<2200*10*.01=220>>220.\n",
      "The total value of the investment for the first year will be $2200 + $220 = $<<2200+220=2420>>2420.\n",
      "For the second year, the total invested will be $2420 + ($100 * 12) = $<<2420+100*12=3620>>3620.\n",
      "The interest calculated after the second year will be $3620 * 10% = $<<3620*10*.01=362>>362.\n",
      "Therefore, Joseph's investment in the mutual fund will be worth $3620 + $362 = $<<3620+362=3982>>3982.\n",
      "#### 3982\n",
      "\n",
      "Question: The price of buying a wooden toy at the new Craftee And Best store is $20, and the cost of buying a hat is $10. If Kendra went to the shop with a $100 bill and bought two wooden toys and three hats, calculate the change she received.\n",
      "Answer: When Kendra bought 2 toys, she paid 2*$20 = $<<2*20=40>>40\n",
      "Since the price of a hat is $10, when Kendra bought 3 hats, she paid 3*$10 = $<<3*10=30>>30\n",
      "The total costs for the hats and wooden toys Kendra bought is $40+$30 = $<<40+30=70>>70\n",
      "From the $100 bill, Kendra received change worth $100-$70 =$<<100-70=30>>30\n",
      "#### 30\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random; rseed = 42; random.seed(rseed)\n",
    "\n",
    "nshot_prompt = f\"\"\n",
    "for top_logit_indices in random.sample(range(len(train_qnas)), 8):\n",
    "    nshot_prompt += f\"Question: {train_qnas[top_logit_indices]['question']}\\nAnswer: {train_qnas[top_logit_indices]['answer']}\\n\\n\"\n",
    "\n",
    "print(nshot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: For every 12 cans you recycle, you receive $0.50, and for every 5 kilograms of newspapers, you receive $1.50. If your family collected 144 cans and 20 kilograms of newspapers, how much money would you receive?\n",
      "Answer: There are 144/12 = <<144/12=12>>12 sets of 12 cans that the family collected.\n",
      "So, the family would receive $0.50 x 12 = $<<0.50*12=6>>6 for the cans.\n",
      "There are 20/5 = <<20/5=4>>4 sets of 5 kilograms of newspapers that the family collected.\n",
      "So, the family would receive $1.50 x 4 = $<<1.50*4=6>>6 for the newspapers.\n",
      "Therefore, the family would receive a total of $6 + $6 = $<<6+6=12>>12.\n",
      "#### 12\n",
      "\n",
      "Question: Betty picked 16 strawberries. Matthew picked 20 more strawberries than Betty and twice as many as Natalie. They used their strawberries to make jam. One jar of jam used 7 strawberries and they sold each jar at $4. How much money were they able to make from the strawberries they picked?\n",
      "Answer: Matthew picked 16 + 20 = <<16+20=36>>36 strawberries.\n",
      "Natalie picked 36/2 = <<36/2=18>>18 strawberries.\n",
      "All together, they have 16 + 36 + 18 = <<16+36+18=70>>70 strawberries.\n",
      "They can make 70/7 = <<70/7=10>>10 jars of strawberries.\n",
      "They earn 10 x $4 = $<<10*4=40>>40 from the strawberries they picked.\n",
      "#### 40\n",
      "\n",
      "Question: Jack has a stack of books that is 12 inches thick. He knows from experience that 80 pages is one inch thick. If he has 6 books, how many pages is each one on average?\n",
      "Answer: There are 960 pages because 80 x 12 = <<80*12=960>>960\n",
      "Each book is 160 pages because 960 / 6 = <<960/6=160>>160\n",
      "#### 160\n",
      "\n",
      "Question: James dumps his whole collection of 500 Legos on the floor and starts building a castle out of them.  He uses half the pieces before finishing and is told to put the rest away.  He puts all of the leftover pieces back in the box they came from, except for 5 missing pieces that he can't find.  How many Legos are in the box at the end?\n",
      "Answer: James starts with 500 Legos and uses half of them, leaving 500/2=<<500/2=250>>250 Legos unused.\n",
      "He puts those unused Legos away but since he's missing 5 he only puts 250-5=<<250-5=245>>245 Legos away.\n",
      "#### 245\n",
      "\n",
      "Question: Ines had $20 in her purse. She bought 3 pounds of peaches, which are $2 per pound at the local farmers’ market. How much did she have left?\n",
      "Answer: Ines bought 3 pounds of peaches for 3 peaches * $2/peach = $<<3*2=6>>6.\n",
      "Ines has $20 - $6 = $<<20-6=14>>14 left.\n",
      "#### 14\n",
      "\n",
      "Question: Aaron pays his actuary membership fees each year. The membership fee increases yearly by $10. If he pays $80 in the first year, how much does his membership cost, in dollars, in the sixth year?\n",
      "Answer: In year 2 he pays 80+10=$<<80+10=90>>90.\n",
      "In year 3 he pays 90+10=$<<90+10=100>>100.\n",
      "In year 4 he pays 100+10=$<<100+10=110>>110.\n",
      "In year 5 he pays 110+10=$<<110+10=120>>120.\n",
      "In year 6 he pays 120+10=$<<120+10=130>>130.\n",
      "#### 130\n",
      "\n",
      "Question: Joseph invested $1000 into a hedge fund. The fund promised a yearly interest rate of 10%. If he deposited an additional $100 every month into the account to add to his initial investment of $1000, how much money will he have in the fund after two years?\n",
      "Answer: For the first year, Joseph will have invested $1000 + ($100 * 12) = $<<1000+100*12=2200>>2200.\n",
      "The interest calculated for the first year will be $2200 * 10% = $<<2200*10*.01=220>>220.\n",
      "The total value of the investment for the first year will be $2200 + $220 = $<<2200+220=2420>>2420.\n",
      "For the second year, the total invested will be $2420 + ($100 * 12) = $<<2420+100*12=3620>>3620.\n",
      "The interest calculated after the second year will be $3620 * 10% = $<<3620*10*.01=362>>362.\n",
      "Therefore, Joseph's investment in the mutual fund will be worth $3620 + $362 = $<<3620+362=3982>>3982.\n",
      "#### 3982\n",
      "\n",
      "Question: The price of buying a wooden toy at the new Craftee And Best store is $20, and the cost of buying a hat is $10. If Kendra went to the shop with a $100 bill and bought two wooden toys and three hats, calculate the change she received.\n",
      "Answer: When Kendra bought 2 toys, she paid 2*$20 = $<<2*20=40>>40\n",
      "Since the price of a hat is $10, when Kendra bought 3 hats, she paid 3*$10 = $<<3*10=30>>30\n",
      "The total costs for the hats and wooden toys Kendra bought is $40+$30 = $<<40+30=70>>70\n",
      "From the $100 bill, Kendra received change worth $100-$70 =$<<100-70=30>>30\n",
      "#### 30\n",
      "\n",
      "Question: John drives for 3 hours at a speed of 60 mph and then turns around because he realizes he forgot something very important at home.  He tries to get home in 4 hours but spends the first 2 hours in standstill traffic.  He spends the next half-hour driving at a speed of 30mph, before being able to drive the remaining time of the 4 hours going at 80 mph.  How far is he from home at the end of those 4 hours? Let's think step by step.\n",
      "Answer: \n",
      "Answer: 45\n",
      "Answer in integer: 45\n"
     ]
    }
   ],
   "source": [
    "def question_to_prompt(question):\n",
    "    return f\"{nshot_prompt}Question: {question} Let's think step by step.\\nAnswer: \"\n",
    "\n",
    "sample_i = 8\n",
    "print(question_to_prompt(test_qnas[sample_i]['question']))\n",
    "\n",
    "from src.util.gsm8k_helper import *\n",
    "print('Answer:', extract_num_from_ans(test_qnas[sample_i]['answer']))\n",
    "print('Answer in integer:', extract_num_from_ans(test_qnas[sample_i]['answer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(input_text, top_k=1):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_length=input_ids.shape[1] + 512,\n",
    "            do_sample=True, top_k=top_k,\n",
    "            eos_token_id=tokenizer.encode(text='\\n\\n', add_special_tokens=False)[0],\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            return_dict_in_generate=True, \n",
    "            output_logits=True, \n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "\n",
    "    output_text = tokenizer.decode(outputs.sequences[0])\n",
    "    generated_answer = output_text.split('Answer: ')[-1].split('\\n\\n')[0]\n",
    "    generated_len = len(outputs.logits)\n",
    "\n",
    "    k = 3\n",
    "    topk_indices = torch.zeros((generated_len, k), dtype=torch.long)\n",
    "    topk_logits = torch.zeros((generated_len, k))\n",
    "    topk_probabilities = torch.zeros((generated_len, k))\n",
    "\n",
    "    # Iterate over each sequence position to find the top-3 indices and their logits and probabilities\n",
    "    for seq_idx, logits_tensor in enumerate(outputs.logits): # outputs.logits: (seq_length, batch_size, vocab_size)\n",
    "        logits = logits_tensor[0]  # score_tensor.shape: (batch_size, vocab_size)\n",
    "        \n",
    "        top_logit_values, top_logit_indices = torch.topk(logits, k=3)\n",
    "        \n",
    "        topk_indices[seq_idx] = top_logit_indices  # Indices of the top-3 tokens\n",
    "        topk_logits[seq_idx] = top_logit_values  # Logits of the top-3 tokens\n",
    "        topk_probabilities[seq_idx] = torch.nn.functional.softmax(logits, dim=-1)[top_logit_indices]  # Probabilities of the top-3 tokens\n",
    "\n",
    "    return {\n",
    "        'generated_answer': generated_answer,\n",
    "        'generated_indices': outputs.sequences[0][input_ids.shape[1]:],\n",
    "        'generated_tokens': [tokenizer.decode(i) for i in outputs.sequences[0][input_ids.shape[1]:]],\n",
    "        'generated_token_len': len(outputs.sequences[0][input_ids.shape[1]:]),\n",
    "        'topk_indices': topk_indices,\n",
    "        'topk_tokens': [[tokenizer.decode(i) for i in row] for row in topk_indices],\n",
    "        'topk_logits': topk_logits,\n",
    "        'topk_probabilities': topk_probabilities,\n",
    "        'vocab_size': outputs.logits[0].shape[-1],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sewoo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:655: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'generated_answer': '180 miles because he was 180 miles from home when he turned around.\\n#### 180',\n",
       " 'generated_indices': tensor([5245, 8931, 1606,  568,  574,  220, 5245, 8931,  505, 2162,  994,  568,\n",
       "         6656, 2212,  627,  827,  220, 5245,  271], device='cuda:0'),\n",
       " 'generated_tokens': ['180',\n",
       "  ' miles',\n",
       "  ' because',\n",
       "  ' he',\n",
       "  ' was',\n",
       "  ' ',\n",
       "  '180',\n",
       "  ' miles',\n",
       "  ' from',\n",
       "  ' home',\n",
       "  ' when',\n",
       "  ' he',\n",
       "  ' turned',\n",
       "  ' around',\n",
       "  '.\\n',\n",
       "  '####',\n",
       "  ' ',\n",
       "  '180',\n",
       "  '\\n\\n'],\n",
       " 'generated_token_len': 19,\n",
       " 'topk_indices': tensor([[  5245,     18,   3842],\n",
       "         [  8931,    198,    374],\n",
       "         [  1606,    374,    505],\n",
       "         [   568,    994,   3842],\n",
       "         [   574,  23980,  31796],\n",
       "         [   220,  10043,  21646],\n",
       "         [  5245,     18,   1399],\n",
       "         [  8931,    994,     14],\n",
       "         [   505,   3201,    994],\n",
       "         [  2162,    813,    279],\n",
       "         [   994,  15453,   1306],\n",
       "         [   568,   3842,    279],\n",
       "         [  6656,   3940,   1176],\n",
       "         [  2212,   1203,    279],\n",
       "         [   627,    323,     13],\n",
       "         [   827,   1548,    791],\n",
       "         [   220,   1774,    320],\n",
       "         [  5245,   1774,     16],\n",
       "         [   271,   4815, 128009]]),\n",
       " 'topk_tokens': [['180', '3', ' John'],\n",
       "  [' miles', '\\n', ' is'],\n",
       "  [' because', ' is', ' from'],\n",
       "  [' he', ' when', ' John'],\n",
       "  [' was', ' drove', ' traveled'],\n",
       "  [' ', ' driving', ' traveling'],\n",
       "  ['180', '3', '60'],\n",
       "  [' miles', ' when', '/'],\n",
       "  [' from', ' away', ' when'],\n",
       "  [' home', ' his', ' the'],\n",
       "  [' when', ' initially', ' after'],\n",
       "  [' he', ' John', ' the'],\n",
       "  [' turned', ' started', ' first'],\n",
       "  [' around', ' back', ' the'],\n",
       "  ['.\\n', ' and', '.'],\n",
       "  ['####', 'He', 'The'],\n",
       "  [' ', '45', ' ('],\n",
       "  ['180', '45', '1'],\n",
       "  ['\\n\\n', ' \\n\\n', '<|eot_id|>']],\n",
       " 'topk_logits': tensor([[17.4375, 15.3906, 15.3516],\n",
       "         [16.3125, 14.8906, 14.0312],\n",
       "         [16.5000, 15.8906, 15.2109],\n",
       "         [17.3281, 16.9062, 16.3906],\n",
       "         [20.1562, 20.1094, 18.6406],\n",
       "         [19.8438, 18.7969, 17.5000],\n",
       "         [22.5000, 20.4375, 19.7344],\n",
       "         [19.5312, 15.8672, 14.3828],\n",
       "         [21.8125, 21.3594, 19.2656],\n",
       "         [23.0938, 20.2812, 20.2500],\n",
       "         [20.8906, 18.4062, 18.2344],\n",
       "         [23.5625, 18.8906, 17.9844],\n",
       "         [21.9375, 19.4062, 18.6094],\n",
       "         [21.9375, 19.0469, 17.7969],\n",
       "         [19.0156, 17.8281, 17.7812],\n",
       "         [17.6562, 13.6953, 12.0312],\n",
       "         [21.4062, 12.8750, 12.4922],\n",
       "         [24.5469, 18.8750, 15.3438],\n",
       "         [21.1562, 18.2188, 15.9219]]),\n",
       " 'topk_probabilities': tensor([[5.2685e-01, 6.8036e-02, 6.5429e-02],\n",
       "         [4.9704e-01, 1.1992e-01, 5.0776e-02],\n",
       "         [2.2888e-01, 1.2444e-01, 6.3062e-02],\n",
       "         [2.7369e-01, 1.7949e-01, 1.0718e-01],\n",
       "         [3.1465e-01, 3.0024e-01, 6.9118e-02],\n",
       "         [5.7723e-01, 2.0263e-01, 5.5395e-02],\n",
       "         [8.2962e-01, 1.0547e-01, 5.2214e-02],\n",
       "         [9.3161e-01, 2.3876e-02, 5.4112e-03],\n",
       "         [5.7488e-01, 3.6542e-01, 4.5028e-02],\n",
       "         [8.8278e-01, 5.3015e-02, 5.1384e-02],\n",
       "         [7.1195e-01, 5.9361e-02, 4.9987e-02],\n",
       "         [9.7964e-01, 9.1643e-03, 3.7027e-03],\n",
       "         [7.9081e-01, 6.2917e-02, 2.8359e-02],\n",
       "         [9.0778e-01, 5.0419e-02, 1.4445e-02],\n",
       "         [4.7425e-01, 1.4464e-01, 1.3801e-01],\n",
       "         [9.5093e-01, 1.8111e-02, 3.4296e-03],\n",
       "         [9.9886e-01, 1.9698e-04, 1.3433e-04],\n",
       "         [9.9566e-01, 3.4265e-03, 1.0029e-04],\n",
       "         [9.3872e-01, 4.9750e-02, 5.0035e-03]]),\n",
       " 'vocab_size': 128256}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(question_to_prompt(test_qnas[sample_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Sample Question: John drives for 3 hours at a speed of 60 mph and then turns around because he realizes he forgot something very important at home.  He tries to get home in 4 hours but spends the first 2 hours in standstill traffic.  He spends the next half-hour driving at a speed of 30mph, before being able to drive the remaining time of the 4 hours going at 80 mph.  How far is he from home at the end of those 4 hours?\n",
      "* Expected Answer: When he turned around he was 3*60=<<3*60=180>>180 miles from home\n",
      "He was only able to drive 4-2=<<4-2=2>>2 hours in the first four hours\n",
      "In half an hour he goes 30*.5=<<30*.5=15>>15 miles\n",
      "He then drives another 2-.5=<<2-.5=1.5>>1.5 hours\n",
      "In that time he goes 80*1.5=<<80*1.5=120>>120 miles\n",
      "So he drove 120+15=<<120+15=135>>135 miles\n",
      "So he is 180-135=<<180-135=45>>45 miles away from home\n",
      "#### 45\n"
     ]
    }
   ],
   "source": [
    "print(\"* Sample Question:\", test_qnas[sample_i]['question'])\n",
    "print(\"* Expected Answer:\", test_qnas[sample_i]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generated_length': 20,\n",
       " 'sum_logits_top1_top2_diff': 49.75,\n",
       " 'avg_logits_top1_top2_diff': 2.4875,\n",
       " 'min_logits_top1_top2_diff': 0.046875,\n",
       " 'sum_logits_top1': 409.46875,\n",
       " 'avg_logits_top1': 20.473438,\n",
       " 'min_logits_top1': 16.3125,\n",
       " 'sum_prob_top1_top2_diff': 11.636504,\n",
       " 'avg_prob_top1_top2_diff': 0.5818252,\n",
       " 'min_prob_top1_top2_diff': 0.014408678,\n",
       " 'sum_prob_top1': 13.93572,\n",
       " 'avg_prob_top1': 0.69678605,\n",
       " 'min_prob_top1': 0.22887741}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_features(ans_data):\n",
    "    top1_probs = ans_data['topk_probabilities'][:, 0].numpy()\n",
    "    top2_probs = ans_data['topk_probabilities'][:, 1].numpy()\n",
    "    top1_logits = ans_data['topk_logits'][:, 0].numpy()\n",
    "    top2_logits = ans_data['topk_logits'][:, 1].numpy()\n",
    "\n",
    "    features = {\n",
    "        'generated_length': ans_data['generated_token_len'],\n",
    "        'sum_logits_top1_top2_diff': (top1_logits - top2_logits).sum(),\n",
    "        'avg_logits_top1_top2_diff': (top1_logits - top2_logits).mean(),\n",
    "        'min_logits_top1_top2_diff': (top1_logits - top2_logits).min(),\n",
    "        'sum_logits_top1': top1_logits.sum(),\n",
    "        'avg_logits_top1': top1_logits.mean(),\n",
    "        'min_logits_top1': top1_logits.min(),\n",
    "        'sum_prob_top1_top2_diff': (top1_probs - top2_probs).sum(),\n",
    "        'avg_prob_top1_top2_diff': (top1_probs - top2_probs).mean(),\n",
    "        'min_prob_top1_top2_diff': (top1_probs - top2_probs).min(),\n",
    "        'sum_prob_top1': top1_probs.sum(),\n",
    "        'avg_prob_top1': top1_probs.mean(),\n",
    "        'min_prob_top1': top1_probs.min(),\n",
    "    }\n",
    "\n",
    "    return features\n",
    "\n",
    "set_seed(42)\n",
    "ans_data = generate_answer(question_to_prompt(test_qnas[sample_i]), top_k=3)\n",
    "get_features(ans_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting Hidden States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:28<00:00,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 10 training samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Collect features and labels from training data\n",
    "train_features = []\n",
    "train_labels = []\n",
    "\n",
    "print(\"Processing training data...\")\n",
    "for i, qna in enumerate(tqdm(train_qnas[:10])): # Change here (e.g., qnas[:20]) for quick testing\n",
    "\n",
    "    ans_data = generate_answer(question_to_prompt(qna['question']), top_k=1)\n",
    "\n",
    "    generated_answer_int = extract_num_from_ans(ans_data['generated_answer'])\n",
    "    ground_truth_int = extract_num_from_ans(qna['answer'])\n",
    "\n",
    "    label = int(generated_answer_int == ground_truth_int)\n",
    "\n",
    "    train_features.append(get_features(ans_data))\n",
    "    train_labels.append(label)\n",
    "\n",
    "print(f\"Collected {len(train_features)} training samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:30<00:00,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 10 test samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Collect features and labels from test data\n",
    "test_features = []\n",
    "test_labels = []\n",
    "\n",
    "print(\"Processing test data...\")\n",
    "for i, qna in enumerate(tqdm(test_qnas[:10])): # Change here (e.g., qnas[:20]) for quick testing\n",
    "\n",
    "    ans_data = generate_answer(question_to_prompt(qna['question']), top_k=1)\n",
    "\n",
    "    generated_answer_int = extract_num_from_ans(ans_data['generated_answer'])\n",
    "    ground_truth_int = extract_num_from_ans(qna['answer'])\n",
    "\n",
    "    label = int(generated_answer_int == ground_truth_int)\n",
    "\n",
    "    test_features.append(get_features(ans_data))\n",
    "    test_labels.append(label)\n",
    "\n",
    "print(f\"Collected {len(test_features)} test samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_length': 52,\n",
       "  'sum_logits_top1_top2_diff': 150.64844,\n",
       "  'avg_logits_top1_top2_diff': 2.8970854,\n",
       "  'min_logits_top1_top2_diff': 0.0,\n",
       "  'sum_logits_top1': 1136.8281,\n",
       "  'avg_logits_top1': 21.86208,\n",
       "  'min_logits_top1': 15.0,\n",
       "  'sum_prob_top1_top2_diff': 35.729748,\n",
       "  'avg_prob_top1_top2_diff': 0.68711054,\n",
       "  'min_prob_top1_top2_diff': 0.0,\n",
       "  'sum_prob_top1': 41.439198,\n",
       "  'avg_prob_top1': 0.79690766,\n",
       "  'min_prob_top1': 0.1745265},\n",
       " {'generated_length': 42,\n",
       "  'sum_logits_top1_top2_diff': 120.25,\n",
       "  'avg_logits_top1_top2_diff': 2.8630953,\n",
       "  'min_logits_top1_top2_diff': 0.09375,\n",
       "  'sum_logits_top1': 906.0156,\n",
       "  'avg_logits_top1': 21.5718,\n",
       "  'min_logits_top1': 16.953125,\n",
       "  'sum_prob_top1_top2_diff': 24.006472,\n",
       "  'avg_prob_top1_top2_diff': 0.5715827,\n",
       "  'min_prob_top1_top2_diff': 0.026295692,\n",
       "  'sum_prob_top1': 28.934895,\n",
       "  'avg_prob_top1': 0.68892604,\n",
       "  'min_prob_top1': 0.20066397},\n",
       " {'generated_length': 116,\n",
       "  'sum_logits_top1_top2_diff': 530.71094,\n",
       "  'avg_logits_top1_top2_diff': 4.575094,\n",
       "  'min_logits_top1_top2_diff': 0.0625,\n",
       "  'sum_logits_top1': 2845.0781,\n",
       "  'avg_logits_top1': 24.526535,\n",
       "  'min_logits_top1': 18.0,\n",
       "  'sum_prob_top1_top2_diff': 86.674355,\n",
       "  'avg_prob_top1_top2_diff': 0.74719274,\n",
       "  'min_prob_top1_top2_diff': 0.02436465,\n",
       "  'sum_prob_top1': 97.29087,\n",
       "  'avg_prob_top1': 0.8387144,\n",
       "  'min_prob_top1': 0.29310948},\n",
       " {'generated_length': 32,\n",
       "  'sum_logits_top1_top2_diff': 89.41406,\n",
       "  'avg_logits_top1_top2_diff': 2.7941895,\n",
       "  'min_logits_top1_top2_diff': 0.015625,\n",
       "  'sum_logits_top1': 666.3281,\n",
       "  'avg_logits_top1': 20.822754,\n",
       "  'min_logits_top1': 16.734375,\n",
       "  'sum_prob_top1_top2_diff': 20.02502,\n",
       "  'avg_prob_top1_top2_diff': 0.6257819,\n",
       "  'min_prob_top1_top2_diff': 0.0031460226,\n",
       "  'sum_prob_top1': 23.584589,\n",
       "  'avg_prob_top1': 0.7370184,\n",
       "  'min_prob_top1': 0.20292254},\n",
       " {'generated_length': 59,\n",
       "  'sum_logits_top1_top2_diff': 288.35938,\n",
       "  'avg_logits_top1_top2_diff': 4.887447,\n",
       "  'min_logits_top1_top2_diff': 0.09375,\n",
       "  'sum_logits_top1': 1375.8281,\n",
       "  'avg_logits_top1': 23.31912,\n",
       "  'min_logits_top1': 16.515625,\n",
       "  'sum_prob_top1_top2_diff': 43.47676,\n",
       "  'avg_prob_top1_top2_diff': 0.73689425,\n",
       "  'min_prob_top1_top2_diff': 0.023715764,\n",
       "  'sum_prob_top1': 48.67048,\n",
       "  'avg_prob_top1': 0.8249234,\n",
       "  'min_prob_top1': 0.26501146},\n",
       " {'generated_length': 108,\n",
       "  'sum_logits_top1_top2_diff': 379.0625,\n",
       "  'avg_logits_top1_top2_diff': 3.5098379,\n",
       "  'min_logits_top1_top2_diff': 0.03125,\n",
       "  'sum_logits_top1': 2533.5938,\n",
       "  'avg_logits_top1': 23.459202,\n",
       "  'min_logits_top1': 18.21875,\n",
       "  'sum_prob_top1_top2_diff': 76.7948,\n",
       "  'avg_prob_top1_top2_diff': 0.71106297,\n",
       "  'min_prob_top1_top2_diff': 0.009704143,\n",
       "  'sum_prob_top1': 88.06461,\n",
       "  'avg_prob_top1': 0.81541306,\n",
       "  'min_prob_top1': 0.23375139},\n",
       " {'generated_length': 47,\n",
       "  'sum_logits_top1_top2_diff': 184.08594,\n",
       "  'avg_logits_top1_top2_diff': 3.916722,\n",
       "  'min_logits_top1_top2_diff': 0.015625,\n",
       "  'sum_logits_top1': 1084.0156,\n",
       "  'avg_logits_top1': 23.064161,\n",
       "  'min_logits_top1': 18.859375,\n",
       "  'sum_prob_top1_top2_diff': 35.679966,\n",
       "  'avg_prob_top1_top2_diff': 0.75914824,\n",
       "  'min_prob_top1_top2_diff': 0.0041226447,\n",
       "  'sum_prob_top1': 39.478233,\n",
       "  'avg_prob_top1': 0.8399624,\n",
       "  'min_prob_top1': 0.215941},\n",
       " {'generated_length': 143,\n",
       "  'sum_logits_top1_top2_diff': 640.7969,\n",
       "  'avg_logits_top1_top2_diff': 4.481097,\n",
       "  'min_logits_top1_top2_diff': 0.0,\n",
       "  'sum_logits_top1': 3451.6562,\n",
       "  'avg_logits_top1': 24.137457,\n",
       "  'min_logits_top1': 16.90625,\n",
       "  'sum_prob_top1_top2_diff': 116.33302,\n",
       "  'avg_prob_top1_top2_diff': 0.81351763,\n",
       "  'min_prob_top1_top2_diff': 0.0,\n",
       "  'sum_prob_top1': 125.647804,\n",
       "  'avg_prob_top1': 0.87865597,\n",
       "  'min_prob_top1': 0.22298741},\n",
       " {'generated_length': 53,\n",
       "  'sum_logits_top1_top2_diff': 262.52344,\n",
       "  'avg_logits_top1_top2_diff': 4.9532723,\n",
       "  'min_logits_top1_top2_diff': 0.1484375,\n",
       "  'sum_logits_top1': 1162.5078,\n",
       "  'avg_logits_top1': 21.934109,\n",
       "  'min_logits_top1': 14.359375,\n",
       "  'sum_prob_top1_top2_diff': 45.268658,\n",
       "  'avg_prob_top1_top2_diff': 0.8541256,\n",
       "  'min_prob_top1_top2_diff': 0.025430366,\n",
       "  'sum_prob_top1': 47.440178,\n",
       "  'avg_prob_top1': 0.8950977,\n",
       "  'min_prob_top1': 0.18435001},\n",
       " {'generated_length': 276,\n",
       "  'sum_logits_top1_top2_diff': 1050.6562,\n",
       "  'avg_logits_top1_top2_diff': 3.8067255,\n",
       "  'min_logits_top1_top2_diff': 0.0,\n",
       "  'sum_logits_top1': 6600.9062,\n",
       "  'avg_logits_top1': 23.916327,\n",
       "  'min_logits_top1': 16.53125,\n",
       "  'sum_prob_top1_top2_diff': 198.78775,\n",
       "  'avg_prob_top1_top2_diff': 0.7202455,\n",
       "  'min_prob_top1_top2_diff': 0.0,\n",
       "  'sum_prob_top1': 224.65274,\n",
       "  'avg_prob_top1': 0.8139592,\n",
       "  'min_prob_top1': 0.15892836}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden State PCA Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_from_dictlist(feature_dictlist, key):\n",
    "    return np.array([f[key] for f in feature_dictlist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_components=2 must be between 0 and min(n_samples, n_features)=1 with svd_solver='covariance_eigh'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 34\u001b[0m\n\u001b[0;32m     30\u001b[0m     plt\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs/2d_pca_visualization-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m---> 34\u001b[0m \u001b[43mplot_2d_pca\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum_logits_top1_top2_diff\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[25], line 8\u001b[0m, in \u001b[0;36mplot_2d_pca\u001b[1;34m(key)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_2d_pca\u001b[39m(key):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# PCA Visualization\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Dimension reduction to 2D using PCA\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m     train_features_2d \u001b[38;5;241m=\u001b[39m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_features\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     test_features_2d \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mtransform([feature[key] \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m train_features])\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Visualize training data\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sewoo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\sewoo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sewoo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:474\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    453\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \n\u001b[0;32m    455\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;124;03m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 474\u001b[0m     U, S, _, X, x_is_centered, xp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m U \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    476\u001b[0m         U \u001b[38;5;241m=\u001b[39m U[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components_]\n",
      "File \u001b[1;32mc:\\Users\\sewoo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:547\u001b[0m, in \u001b[0;36mPCA._fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;66;03m# Call different fits for either full or truncated SVD\u001b[39;00m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcovariance_eigh\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 547\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_array_api_compliant\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandomized\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_truncated(X, n_components, xp)\n",
      "File \u001b[1;32mc:\\Users\\sewoo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:561\u001b[0m, in \u001b[0;36mPCA._fit_full\u001b[1;34m(self, X, n_components, xp, is_array_api_compliant)\u001b[0m\n\u001b[0;32m    557\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    558\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_components=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmle\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is only supported if n_samples >= n_features\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    559\u001b[0m         )\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m n_components \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(n_samples, n_features):\n\u001b[1;32m--> 561\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    562\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_components=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_components\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be between 0 and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    563\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin(n_samples, n_features)=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmin\u001b[39m(n_samples,\u001b[38;5;250m \u001b[39mn_features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    564\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msvd_solver=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    565\u001b[0m     )\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_ \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mmean(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    568\u001b[0m \u001b[38;5;66;03m# When X is a scipy sparse matrix, self.mean_ is a numpy matrix, so we need\u001b[39;00m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;66;03m# to transform it to a 1D array. Note that this is not the case when X\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;66;03m# is a scipy sparse array.\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;66;03m# TODO: remove the following two lines when scikit-learn only depends\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;66;03m# on scipy versions that no longer support scipy.sparse matrices.\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: n_components=2 must be between 0 and min(n_samples, n_features)=1 with svd_solver='covariance_eigh'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def plot_2d_pca(key):\n",
    "    # PCA Visualization\n",
    "    # Dimension reduction to 2D using PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    train_features_2d = pca.fit_transform(get_features_from_dictlist(train_features, key))\n",
    "    test_features_2d = pca.transform(get_features_from_dictlist(test_features, key))\n",
    "\n",
    "    # Visualize training data\n",
    "    plt.scatter(train_features_2d[train_labels == 0][:, 0], train_features_2d[train_labels == 0][:, 1], \n",
    "                label=\"Train - Incorrect\", marker='x', alpha=0.5)\n",
    "    plt.scatter(train_features_2d[train_labels == 1][:, 0], train_features_2d[train_labels == 1][:, 1], \n",
    "                label=\"Train - Correct\", marker='o', alpha=0.5)\n",
    "\n",
    "    # Visualize test data\n",
    "    plt.scatter(test_features_2d[test_labels == 0][:, 0], test_features_2d[test_labels == 0][:, 1], \n",
    "                label=\"Test - Incorrect\", marker='x', alpha=0.7)\n",
    "    plt.scatter(test_features_2d[test_labels == 1][:, 0], test_features_2d[test_labels == 1][:, 1], \n",
    "                label=\"Test - Correct\", marker='o', alpha=0.7)\n",
    "\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "    import os\n",
    "    os.makedirs(\"outputs\", exist_ok=True)\n",
    "    plt.savefig(f\"outputs/2d_pca_visualization-{model_name.split('/')[-1]}.pdf\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_2d_pca('sum_logits_top1_top2_diff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# 3D PCA-based Visualization with different markers for correct and incorrect\n",
    "pca_3d = PCA(n_components=3)\n",
    "train_features_3d = pca_3d.fit_transform(train_features)\n",
    "test_features_3d = pca_3d.transform(test_features)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Visualize training data in 3D\n",
    "ax.scatter(train_features_3d[train_labels == 0][:, 0], train_features_3d[train_labels == 0][:, 1], train_features_3d[train_labels == 0][:, 2], \n",
    "           label=\"Train - Incorrect\", marker='x', alpha=0.5)\n",
    "ax.scatter(train_features_3d[train_labels == 1][:, 0], train_features_3d[train_labels == 1][:, 1], train_features_3d[train_labels == 1][:, 2], \n",
    "           label=\"Train - Correct\", marker='o', alpha=0.5)\n",
    "\n",
    "# Visualize test data in 3D\n",
    "ax.scatter(test_features_3d[test_labels == 0][:, 0], test_features_3d[test_labels == 0][:, 1], test_features_3d[test_labels == 0][:, 2], \n",
    "           label=\"Test - Incorrect\", marker='x', alpha=0.7)\n",
    "ax.scatter(test_features_3d[test_labels == 1][:, 0], test_features_3d[test_labels == 1][:, 1], test_features_3d[test_labels == 1][:, 2], \n",
    "           label=\"Test - Correct\", marker='o', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel(\"PC 1\")\n",
    "ax.set_ylabel(\"PC 2\")\n",
    "ax.set_zlabel(\"PC 3\")\n",
    "ax.set_title(\"3D PCA Visualization of Training and Test Features\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
