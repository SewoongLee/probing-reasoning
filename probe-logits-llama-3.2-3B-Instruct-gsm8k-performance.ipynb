{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sewoo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:777: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc5be6698a914e04aa26494fe7a24671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s],))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 3072)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
      ") LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "# model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "# model_name = \"meta-llama/Llama-3.2-3B\"\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "# model_name = \"google/gemma-2-2b\"\n",
    "# model_name = \"google/gemma-2-2b-it\"\n",
    "# model_name = \"google/gemma-2-9b\"\n",
    "# model_name = \"google/gemma-2-9b-it\"\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    output_hidden_states=True,  # Enable hidden states\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "print(model, model.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7473, 1319)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.util.json_io import *\n",
    "\n",
    "train_qnas = load_jsonlines(f'data/gsm8k/train.jsonl')\n",
    "test_qnas = load_jsonlines(f'data/gsm8k/test.jsonl')\n",
    "len(train_qnas), len(test_qnas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: For every 12 cans you recycle, you receive $0.50, and for every 5 kilograms of newspapers, you receive $1.50. If your family collected 144 cans and 20 kilograms of newspapers, how much money would you receive?\n",
      "Answer: There are 144/12 = <<144/12=12>>12 sets of 12 cans that the family collected.\n",
      "So, the family would receive $0.50 x 12 = $<<0.50*12=6>>6 for the cans.\n",
      "There are 20/5 = <<20/5=4>>4 sets of 5 kilograms of newspapers that the family collected.\n",
      "So, the family would receive $1.50 x 4 = $<<1.50*4=6>>6 for the newspapers.\n",
      "Therefore, the family would receive a total of $6 + $6 = $<<6+6=12>>12.\n",
      "#### 12\n",
      "\n",
      "Question: Betty picked 16 strawberries. Matthew picked 20 more strawberries than Betty and twice as many as Natalie. They used their strawberries to make jam. One jar of jam used 7 strawberries and they sold each jar at $4. How much money were they able to make from the strawberries they picked?\n",
      "Answer: Matthew picked 16 + 20 = <<16+20=36>>36 strawberries.\n",
      "Natalie picked 36/2 = <<36/2=18>>18 strawberries.\n",
      "All together, they have 16 + 36 + 18 = <<16+36+18=70>>70 strawberries.\n",
      "They can make 70/7 = <<70/7=10>>10 jars of strawberries.\n",
      "They earn 10 x $4 = $<<10*4=40>>40 from the strawberries they picked.\n",
      "#### 40\n",
      "\n",
      "Question: Jack has a stack of books that is 12 inches thick. He knows from experience that 80 pages is one inch thick. If he has 6 books, how many pages is each one on average?\n",
      "Answer: There are 960 pages because 80 x 12 = <<80*12=960>>960\n",
      "Each book is 160 pages because 960 / 6 = <<960/6=160>>160\n",
      "#### 160\n",
      "\n",
      "Question: James dumps his whole collection of 500 Legos on the floor and starts building a castle out of them.  He uses half the pieces before finishing and is told to put the rest away.  He puts all of the leftover pieces back in the box they came from, except for 5 missing pieces that he can't find.  How many Legos are in the box at the end?\n",
      "Answer: James starts with 500 Legos and uses half of them, leaving 500/2=<<500/2=250>>250 Legos unused.\n",
      "He puts those unused Legos away but since he's missing 5 he only puts 250-5=<<250-5=245>>245 Legos away.\n",
      "#### 245\n",
      "\n",
      "Question: Ines had $20 in her purse. She bought 3 pounds of peaches, which are $2 per pound at the local farmersâ€™ market. How much did she have left?\n",
      "Answer: Ines bought 3 pounds of peaches for 3 peaches * $2/peach = $<<3*2=6>>6.\n",
      "Ines has $20 - $6 = $<<20-6=14>>14 left.\n",
      "#### 14\n",
      "\n",
      "Question: Aaron pays his actuary membership fees each year. The membership fee increases yearly by $10. If he pays $80 in the first year, how much does his membership cost, in dollars, in the sixth year?\n",
      "Answer: In year 2 he pays 80+10=$<<80+10=90>>90.\n",
      "In year 3 he pays 90+10=$<<90+10=100>>100.\n",
      "In year 4 he pays 100+10=$<<100+10=110>>110.\n",
      "In year 5 he pays 110+10=$<<110+10=120>>120.\n",
      "In year 6 he pays 120+10=$<<120+10=130>>130.\n",
      "#### 130\n",
      "\n",
      "Question: Joseph invested $1000 into a hedge fund. The fund promised a yearly interest rate of 10%. If he deposited an additional $100 every month into the account to add to his initial investment of $1000, how much money will he have in the fund after two years?\n",
      "Answer: For the first year, Joseph will have invested $1000 + ($100 * 12) = $<<1000+100*12=2200>>2200.\n",
      "The interest calculated for the first year will be $2200 * 10% = $<<2200*10*.01=220>>220.\n",
      "The total value of the investment for the first year will be $2200 + $220 = $<<2200+220=2420>>2420.\n",
      "For the second year, the total invested will be $2420 + ($100 * 12) = $<<2420+100*12=3620>>3620.\n",
      "The interest calculated after the second year will be $3620 * 10% = $<<3620*10*.01=362>>362.\n",
      "Therefore, Joseph's investment in the mutual fund will be worth $3620 + $362 = $<<3620+362=3982>>3982.\n",
      "#### 3982\n",
      "\n",
      "Question: The price of buying a wooden toy at the new Craftee And Best store is $20, and the cost of buying a hat is $10. If Kendra went to the shop with a $100 bill and bought two wooden toys and three hats, calculate the change she received.\n",
      "Answer: When Kendra bought 2 toys, she paid 2*$20 = $<<2*20=40>>40\n",
      "Since the price of a hat is $10, when Kendra bought 3 hats, she paid 3*$10 = $<<3*10=30>>30\n",
      "The total costs for the hats and wooden toys Kendra bought is $40+$30 = $<<40+30=70>>70\n",
      "From the $100 bill, Kendra received change worth $100-$70 =$<<100-70=30>>30\n",
      "#### 30\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random; rseed = 42; random.seed(rseed)\n",
    "\n",
    "nshot_prompt = f\"\"\n",
    "for top_logit_indices in random.sample(range(len(train_qnas)), 8):\n",
    "    nshot_prompt += f\"Question: {train_qnas[top_logit_indices]['question']}\\nAnswer: {train_qnas[top_logit_indices]['answer']}\\n\\n\"\n",
    "\n",
    "print(nshot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: For every 12 cans you recycle, you receive $0.50, and for every 5 kilograms of newspapers, you receive $1.50. If your family collected 144 cans and 20 kilograms of newspapers, how much money would you receive?\n",
      "Answer: There are 144/12 = <<144/12=12>>12 sets of 12 cans that the family collected.\n",
      "So, the family would receive $0.50 x 12 = $<<0.50*12=6>>6 for the cans.\n",
      "There are 20/5 = <<20/5=4>>4 sets of 5 kilograms of newspapers that the family collected.\n",
      "So, the family would receive $1.50 x 4 = $<<1.50*4=6>>6 for the newspapers.\n",
      "Therefore, the family would receive a total of $6 + $6 = $<<6+6=12>>12.\n",
      "#### 12\n",
      "\n",
      "Question: Betty picked 16 strawberries. Matthew picked 20 more strawberries than Betty and twice as many as Natalie. They used their strawberries to make jam. One jar of jam used 7 strawberries and they sold each jar at $4. How much money were they able to make from the strawberries they picked?\n",
      "Answer: Matthew picked 16 + 20 = <<16+20=36>>36 strawberries.\n",
      "Natalie picked 36/2 = <<36/2=18>>18 strawberries.\n",
      "All together, they have 16 + 36 + 18 = <<16+36+18=70>>70 strawberries.\n",
      "They can make 70/7 = <<70/7=10>>10 jars of strawberries.\n",
      "They earn 10 x $4 = $<<10*4=40>>40 from the strawberries they picked.\n",
      "#### 40\n",
      "\n",
      "Question: Jack has a stack of books that is 12 inches thick. He knows from experience that 80 pages is one inch thick. If he has 6 books, how many pages is each one on average?\n",
      "Answer: There are 960 pages because 80 x 12 = <<80*12=960>>960\n",
      "Each book is 160 pages because 960 / 6 = <<960/6=160>>160\n",
      "#### 160\n",
      "\n",
      "Question: James dumps his whole collection of 500 Legos on the floor and starts building a castle out of them.  He uses half the pieces before finishing and is told to put the rest away.  He puts all of the leftover pieces back in the box they came from, except for 5 missing pieces that he can't find.  How many Legos are in the box at the end?\n",
      "Answer: James starts with 500 Legos and uses half of them, leaving 500/2=<<500/2=250>>250 Legos unused.\n",
      "He puts those unused Legos away but since he's missing 5 he only puts 250-5=<<250-5=245>>245 Legos away.\n",
      "#### 245\n",
      "\n",
      "Question: Ines had $20 in her purse. She bought 3 pounds of peaches, which are $2 per pound at the local farmersâ€™ market. How much did she have left?\n",
      "Answer: Ines bought 3 pounds of peaches for 3 peaches * $2/peach = $<<3*2=6>>6.\n",
      "Ines has $20 - $6 = $<<20-6=14>>14 left.\n",
      "#### 14\n",
      "\n",
      "Question: Aaron pays his actuary membership fees each year. The membership fee increases yearly by $10. If he pays $80 in the first year, how much does his membership cost, in dollars, in the sixth year?\n",
      "Answer: In year 2 he pays 80+10=$<<80+10=90>>90.\n",
      "In year 3 he pays 90+10=$<<90+10=100>>100.\n",
      "In year 4 he pays 100+10=$<<100+10=110>>110.\n",
      "In year 5 he pays 110+10=$<<110+10=120>>120.\n",
      "In year 6 he pays 120+10=$<<120+10=130>>130.\n",
      "#### 130\n",
      "\n",
      "Question: Joseph invested $1000 into a hedge fund. The fund promised a yearly interest rate of 10%. If he deposited an additional $100 every month into the account to add to his initial investment of $1000, how much money will he have in the fund after two years?\n",
      "Answer: For the first year, Joseph will have invested $1000 + ($100 * 12) = $<<1000+100*12=2200>>2200.\n",
      "The interest calculated for the first year will be $2200 * 10% = $<<2200*10*.01=220>>220.\n",
      "The total value of the investment for the first year will be $2200 + $220 = $<<2200+220=2420>>2420.\n",
      "For the second year, the total invested will be $2420 + ($100 * 12) = $<<2420+100*12=3620>>3620.\n",
      "The interest calculated after the second year will be $3620 * 10% = $<<3620*10*.01=362>>362.\n",
      "Therefore, Joseph's investment in the mutual fund will be worth $3620 + $362 = $<<3620+362=3982>>3982.\n",
      "#### 3982\n",
      "\n",
      "Question: The price of buying a wooden toy at the new Craftee And Best store is $20, and the cost of buying a hat is $10. If Kendra went to the shop with a $100 bill and bought two wooden toys and three hats, calculate the change she received.\n",
      "Answer: When Kendra bought 2 toys, she paid 2*$20 = $<<2*20=40>>40\n",
      "Since the price of a hat is $10, when Kendra bought 3 hats, she paid 3*$10 = $<<3*10=30>>30\n",
      "The total costs for the hats and wooden toys Kendra bought is $40+$30 = $<<40+30=70>>70\n",
      "From the $100 bill, Kendra received change worth $100-$70 =$<<100-70=30>>30\n",
      "#### 30\n",
      "\n",
      "Question: Kylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second glass costs only 60% of the price. Kylar wants to buy 16 glasses. How much does he need to pay for them? Let's think step by step.\n",
      "Answer: \n",
      "Answer: 64\n",
      "Answer in integer: 64\n"
     ]
    }
   ],
   "source": [
    "def question_to_prompt(question):\n",
    "    return f\"{nshot_prompt}Question: {question} Let's think step by step.\\nAnswer: \"\n",
    "\n",
    "sample_i = 5\n",
    "print(question_to_prompt(test_qnas[sample_i]['question']))\n",
    "\n",
    "from src.util.gsm8k_helper import *\n",
    "print('Answer:', extract_num_from_ans(test_qnas[sample_i]['answer']))\n",
    "print('Answer in integer:', extract_num_from_ans(test_qnas[sample_i]['answer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_length=input_ids.shape[1] + 512,\n",
    "            do_sample=True, top_k=1,\n",
    "            # temperature=1e-4,\n",
    "            eos_token_id=tokenizer.encode(text='\\n\\n', add_special_tokens=False)[0],\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            return_dict_in_generate=True, \n",
    "            output_logits=True, \n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "\n",
    "    output_text = tokenizer.decode(outputs.sequences[0])\n",
    "    generated_answer = output_text.split('Answer: ')[-1].split('\\n\\n')[0]\n",
    "    generated_len = len(outputs.logits)\n",
    "\n",
    "    k = 3\n",
    "    topk_indices = torch.zeros((generated_len, k), dtype=torch.long)\n",
    "    topk_logits = torch.zeros((generated_len, k))\n",
    "    topk_probabilities = torch.zeros((generated_len, k))\n",
    "\n",
    "    # Iterate over each sequence position to find the top-3 indices and their logits and probabilities\n",
    "    for seq_idx, logits_tensor in enumerate(outputs.logits): # outputs.logits: (seq_length, batch_size, vocab_size)\n",
    "        logits = logits_tensor[0]  # score_tensor.shape: (batch_size, vocab_size)\n",
    "        \n",
    "        top_logit_values, top_logit_indices = torch.topk(logits, k=3)\n",
    "        \n",
    "        topk_indices[seq_idx] = top_logit_indices  # Indices of the top-3 tokens\n",
    "        topk_logits[seq_idx] = top_logit_values  # Logits of the top-3 tokens\n",
    "        topk_probabilities[seq_idx] = torch.nn.functional.softmax(logits, dim=-1)[top_logit_indices]  # Probabilities of the top-3 tokens\n",
    "\n",
    "    return {\n",
    "        'generated_answer': generated_answer,\n",
    "        'generated_indices': outputs.sequences[0][input_ids.shape[1]:],\n",
    "        'generated_tokens': [tokenizer.decode(i) for i in outputs.sequences[0][input_ids.shape[1]:]],\n",
    "        'generated_token_len': len(outputs.sequences[0][input_ids.shape[1]:]),\n",
    "        'topk_indices': topk_indices,\n",
    "        'topk_tokens': [[tokenizer.decode(i) for i in row] for row in topk_indices],\n",
    "        'topk_logits': topk_logits,\n",
    "        'topk_probabilities': topk_probabilities,\n",
    "        'vocab_size': outputs.logits[0].shape[-1],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generated_answer': '60% of the price of one glass is 60/100 * 5 = $<<60/100*5=3>>3.\\nKylar will buy 8 glasses at the discounted price of $3 each, so he will pay 8 * $3 = $<<8*3=24>>24 for those glasses.\\nKylar will buy 8 glasses at the regular price of $5 each, so he will pay 8 * $5 = $<<8*5=40>>40 for those glasses.\\nIn total, Kylar will pay $24 + $40 = $<<24+40=64>>64 for the glasses.\\n#### 64',\n",
       " 'generated_indices': tensor([ 1399,     4,   315,   279,  3430,   315,   832,  9168,   374,   220,\n",
       "          1399,    14,  1041,   353,   220,    20,   284,   400,  2501,  1399,\n",
       "            14,  1041,     9,    20,    28,    18,  2511,    18,   627,    42,\n",
       "          4010,   277,   690,  3780,   220,    23, 29247,   520,   279, 48774,\n",
       "          3430,   315,   400,    18,  1855,    11,   779,   568,   690,  2343,\n",
       "           220,    23,   353,   400,    18,   284,   400,  2501,    23,     9,\n",
       "            18,    28,  1187,  2511,  1187,   369,  1884, 29247,   627,    42,\n",
       "          4010,   277,   690,  3780,   220,    23, 29247,   520,   279,  5912,\n",
       "          3430,   315,   400,    20,  1855,    11,   779,   568,   690,  2343,\n",
       "           220,    23,   353,   400,    20,   284,   400,  2501,    23,     9,\n",
       "            20,    28,  1272,  2511,  1272,   369,  1884, 29247,   627,   644,\n",
       "          2860,    11,   735,  4010,   277,   690,  2343,   400,  1187,   489,\n",
       "           400,  1272,   284,   400,  2501,  1187,    10,  1272,    28,  1227,\n",
       "          2511,  1227,   369,   279, 29247,   627,   827,   220,  1227,   271],\n",
       "        device='cuda:0'),\n",
       " 'generated_tokens': ['60',\n",
       "  '%',\n",
       "  ' of',\n",
       "  ' the',\n",
       "  ' price',\n",
       "  ' of',\n",
       "  ' one',\n",
       "  ' glass',\n",
       "  ' is',\n",
       "  ' ',\n",
       "  '60',\n",
       "  '/',\n",
       "  '100',\n",
       "  ' *',\n",
       "  ' ',\n",
       "  '5',\n",
       "  ' =',\n",
       "  ' $',\n",
       "  '<<',\n",
       "  '60',\n",
       "  '/',\n",
       "  '100',\n",
       "  '*',\n",
       "  '5',\n",
       "  '=',\n",
       "  '3',\n",
       "  '>>',\n",
       "  '3',\n",
       "  '.\\n',\n",
       "  'K',\n",
       "  'yl',\n",
       "  'ar',\n",
       "  ' will',\n",
       "  ' buy',\n",
       "  ' ',\n",
       "  '8',\n",
       "  ' glasses',\n",
       "  ' at',\n",
       "  ' the',\n",
       "  ' discounted',\n",
       "  ' price',\n",
       "  ' of',\n",
       "  ' $',\n",
       "  '3',\n",
       "  ' each',\n",
       "  ',',\n",
       "  ' so',\n",
       "  ' he',\n",
       "  ' will',\n",
       "  ' pay',\n",
       "  ' ',\n",
       "  '8',\n",
       "  ' *',\n",
       "  ' $',\n",
       "  '3',\n",
       "  ' =',\n",
       "  ' $',\n",
       "  '<<',\n",
       "  '8',\n",
       "  '*',\n",
       "  '3',\n",
       "  '=',\n",
       "  '24',\n",
       "  '>>',\n",
       "  '24',\n",
       "  ' for',\n",
       "  ' those',\n",
       "  ' glasses',\n",
       "  '.\\n',\n",
       "  'K',\n",
       "  'yl',\n",
       "  'ar',\n",
       "  ' will',\n",
       "  ' buy',\n",
       "  ' ',\n",
       "  '8',\n",
       "  ' glasses',\n",
       "  ' at',\n",
       "  ' the',\n",
       "  ' regular',\n",
       "  ' price',\n",
       "  ' of',\n",
       "  ' $',\n",
       "  '5',\n",
       "  ' each',\n",
       "  ',',\n",
       "  ' so',\n",
       "  ' he',\n",
       "  ' will',\n",
       "  ' pay',\n",
       "  ' ',\n",
       "  '8',\n",
       "  ' *',\n",
       "  ' $',\n",
       "  '5',\n",
       "  ' =',\n",
       "  ' $',\n",
       "  '<<',\n",
       "  '8',\n",
       "  '*',\n",
       "  '5',\n",
       "  '=',\n",
       "  '40',\n",
       "  '>>',\n",
       "  '40',\n",
       "  ' for',\n",
       "  ' those',\n",
       "  ' glasses',\n",
       "  '.\\n',\n",
       "  'In',\n",
       "  ' total',\n",
       "  ',',\n",
       "  ' K',\n",
       "  'yl',\n",
       "  'ar',\n",
       "  ' will',\n",
       "  ' pay',\n",
       "  ' $',\n",
       "  '24',\n",
       "  ' +',\n",
       "  ' $',\n",
       "  '40',\n",
       "  ' =',\n",
       "  ' $',\n",
       "  '<<',\n",
       "  '24',\n",
       "  '+',\n",
       "  '40',\n",
       "  '=',\n",
       "  '64',\n",
       "  '>>',\n",
       "  '64',\n",
       "  ' for',\n",
       "  ' the',\n",
       "  ' glasses',\n",
       "  '.\\n',\n",
       "  '####',\n",
       "  ' ',\n",
       "  '64',\n",
       "  '\\n\\n'],\n",
       " 'generated_token_len': 140,\n",
       " 'topk_indices': tensor([[  1399,     23,    578],\n",
       "         [     4,     14,   1034],\n",
       "         [   315,    374,    284],\n",
       "         [   279,    400,    832],\n",
       "         [  3430,   2853,   4113],\n",
       "         [   315,    374,    369],\n",
       "         [   832,    264,    220],\n",
       "         [  9168,    400,  29247],\n",
       "         [   374,   7194,    315],\n",
       "         [   220,    400,    662],\n",
       "         [  1399,     15,     20],\n",
       "         [    14,      4,    611],\n",
       "         [  1041,    220,    605],\n",
       "         [   353,  40118,      9],\n",
       "         [   220,    400,      3],\n",
       "         [    20,     19,     18],\n",
       "         [   284,  47481,  11441],\n",
       "         [   400,    220,   1134],\n",
       "         [  2501,     18,     27],\n",
       "         [  1399,    966,     20],\n",
       "         [    14,      4,  12148],\n",
       "         [  1041,    605,     16],\n",
       "         [     9,    353,  20517],\n",
       "         [    20,    220,     18],\n",
       "         [    28,    284,     13],\n",
       "         [    18,    220,     17],\n",
       "         [  2511,  82460,     29],\n",
       "         [    18,    220,  28956],\n",
       "         [   627,    198,     11],\n",
       "         [    42,  12834,   2746],\n",
       "         [  4010,  61799,  61822],\n",
       "         [   277,    269,   1590],\n",
       "         [   690,    374,  50631],\n",
       "         [  3780,   2343,    617],\n",
       "         [   220,   4376,   1403],\n",
       "         [    23,    845,     17],\n",
       "         [ 29247,  23917,    315],\n",
       "         [   520,    430,    369],\n",
       "         [   279,    400,    420],\n",
       "         [ 48774,  11336,  23917],\n",
       "         [  3430,   4478,   7729],\n",
       "         [   315,    323,     11],\n",
       "         [   400,    220,   2380],\n",
       "         [    18,   2501,    220],\n",
       "         [  1855,    323,     11],\n",
       "         [    11,     13,    627],\n",
       "         [   779,    369,    902],\n",
       "         [   568,    430,    279],\n",
       "         [   690,   3358,  21935],\n",
       "         [  2343,   8493,    617],\n",
       "         [   220,    369,    400],\n",
       "         [    23,     18,   1187],\n",
       "         [   353,    865,  40118],\n",
       "         [   400,    220,      3],\n",
       "         [    18,   2501,    220],\n",
       "         [   284,  47481,     28],\n",
       "         [   400,    220,   1134],\n",
       "         [  2501,   1187,     27],\n",
       "         [    23,     18,   1187],\n",
       "         [     9,    353,  40118],\n",
       "         [    18,     17,     19],\n",
       "         [    28,    284,     10],\n",
       "         [  1187,    717,   1419],\n",
       "         [  2511,     29,   3662],\n",
       "         [  1187,   2000,    220],\n",
       "         [   369,    627,    198],\n",
       "         [  1884,   1521,    279],\n",
       "         [ 29247,    627,    220],\n",
       "         [   627,    198,     11],\n",
       "         [    42,   1548,   2520],\n",
       "         [  4010,  61799,  61822],\n",
       "         [   277,   1590,    269],\n",
       "         [   690,   1101,    374],\n",
       "         [  3780,   1101,   2343],\n",
       "         [   220,    279,   2500],\n",
       "         [    23,    845,     19],\n",
       "         [ 29247,   5912,    810],\n",
       "         [   520,    430,    369],\n",
       "         [   279,   2539,   5912],\n",
       "         [  5912,   2539,   3430],\n",
       "         [  3430,  33705,  48774],\n",
       "         [   315,     11,     13],\n",
       "         [   400,    220,    832],\n",
       "         [    20,     21,     19],\n",
       "         [  1855,     11,    824],\n",
       "         [    11,     13,    779],\n",
       "         [   779,    369,    902],\n",
       "         [   568,    735,    430],\n",
       "         [   690,   3358,  21935],\n",
       "         [  2343,   8493,    617],\n",
       "         [   220,    400,    369],\n",
       "         [    23,     20,   1187],\n",
       "         [   353,    865,      9],\n",
       "         [   400,    220,      3],\n",
       "         [    20,   2501,     18],\n",
       "         [   284,  47481,     28],\n",
       "         [   400,    220,   1134],\n",
       "         [  2501,   1272,     27],\n",
       "         [    23,   1187,     20],\n",
       "         [     9,    353,  40118],\n",
       "         [    20,     18,     19],\n",
       "         [    28,    284,     12],\n",
       "         [  1272,   3443,   2137],\n",
       "         [  2511,   3662,     29],\n",
       "         [  1272,    220,    966],\n",
       "         [   369,    627,    198],\n",
       "         [  1884,   1521,    279],\n",
       "         [ 29247,   5912,    220],\n",
       "         [   627,    198,    382],\n",
       "         [   644,     42,    791],\n",
       "         [  2860,    682,   2015],\n",
       "         [    11,    735,    568],\n",
       "         [   735,    568,    279],\n",
       "         [  4010,  61822,  11160],\n",
       "         [   277,   1590,   5169],\n",
       "         [   690,   3966,    706],\n",
       "         [  2343,   1205,    617],\n",
       "         [   400,    220,    369],\n",
       "         [  1187,   1272,     18],\n",
       "         [   489,  32819,    320],\n",
       "         [   400,    220,      3],\n",
       "         [  1272,    220,   2501],\n",
       "         [   284,  47481,     28],\n",
       "         [   400,    220,   1134],\n",
       "         [  2501,   1227,     27],\n",
       "         [  1187,   1227,     23],\n",
       "         [    10,    489,      9],\n",
       "         [  1272,    220,    966],\n",
       "         [    28,    284,    429],\n",
       "         [  1227,  20478,  10513],\n",
       "         [  2511,     29,   3662],\n",
       "         [  1227,   2397,   1399],\n",
       "         [   369,    627,    198],\n",
       "         [   279,    682,    220],\n",
       "         [ 29247,    220,   9168],\n",
       "         [   627,    568,    198],\n",
       "         [   827,    791,   4516],\n",
       "         [   220,    400,   1227],\n",
       "         [  1227,    220,   5538],\n",
       "         [   271,   4815, 128009]]),\n",
       " 'topk_tokens': [['60', '8', ' The'],\n",
       "  ['%', '/', ' %'],\n",
       "  [' of', ' is', ' ='],\n",
       "  [' the', ' $', ' one'],\n",
       "  [' price', ' cost', ' original'],\n",
       "  [' of', ' is', ' for'],\n",
       "  [' one', ' a', ' '],\n",
       "  [' glass', ' $', ' glasses'],\n",
       "  [' is', ' costs', ' of'],\n",
       "  [' ', ' $', '.'],\n",
       "  ['60', '0', '5'],\n",
       "  ['/', '%', ' /'],\n",
       "  ['100', ' ', '10'],\n",
       "  [' *', '*$', '*'],\n",
       "  [' ', ' $', '$'],\n",
       "  ['5', '4', '3'],\n",
       "  [' =', ' =$', ' dollars'],\n",
       "  [' $', ' ', ' <<'],\n",
       "  ['<<', '3', '<'],\n",
       "  ['60', '30', '5'],\n",
       "  ['/', '%', '/('],\n",
       "  ['100', '10', '1'],\n",
       "  ['*', ' *', '*.'],\n",
       "  ['5', ' ', '3'],\n",
       "  ['=', ' =', '.'],\n",
       "  ['3', ' ', '2'],\n",
       "  ['>>', '>>,', '>'],\n",
       "  ['3', ' ', 'three'],\n",
       "  ['.\\n', '\\n', ','],\n",
       "  ['K', 'Since', 'If'],\n",
       "  ['yl', 'endra', 'yla'],\n",
       "  ['ar', 'or', 'ars'],\n",
       "  [' will', ' is', ' buys'],\n",
       "  [' buy', ' pay', ' have'],\n",
       "  [' ', ' half', ' two'],\n",
       "  ['8', '16', '2'],\n",
       "  [' glasses', ' cheaper', ' of'],\n",
       "  [' at', ' that', ' for'],\n",
       "  [' the', ' $', ' this'],\n",
       "  [' discounted', ' discount', ' cheaper'],\n",
       "  [' price', ' rate', ' prices'],\n",
       "  [' of', ' and', ','],\n",
       "  [' $', ' ', ' three'],\n",
       "  ['3', '<<', ' '],\n",
       "  [' each', ' and', ','],\n",
       "  [',', '.', '.\\n'],\n",
       "  [' so', ' for', ' which'],\n",
       "  [' he', ' that', ' the'],\n",
       "  [' will', \"'ll\", ' pays'],\n",
       "  [' pay', ' spend', ' have'],\n",
       "  [' ', ' for', ' $'],\n",
       "  ['8', '3', '24'],\n",
       "  [' *', ' x', '*$'],\n",
       "  [' $', ' ', '$'],\n",
       "  ['3', '<<', ' '],\n",
       "  [' =', ' =$', '='],\n",
       "  [' $', ' ', ' <<'],\n",
       "  ['<<', '24', '<'],\n",
       "  ['8', '3', '24'],\n",
       "  ['*', ' *', '*$'],\n",
       "  ['3', '2', '4'],\n",
       "  ['=', ' =', '+'],\n",
       "  ['24', '12', '23'],\n",
       "  ['>>', '>', ' >>'],\n",
       "  ['24', 'for', ' '],\n",
       "  [' for', '.\\n', '\\n'],\n",
       "  [' those', ' these', ' the'],\n",
       "  [' glasses', '.\\n', ' '],\n",
       "  ['.\\n', '\\n', ','],\n",
       "  ['K', 'He', 'For'],\n",
       "  ['yl', 'endra', 'yla'],\n",
       "  ['ar', 'ars', 'or'],\n",
       "  [' will', ' also', ' is'],\n",
       "  [' buy', ' also', ' pay'],\n",
       "  [' ', ' the', ' another'],\n",
       "  ['8', '16', '4'],\n",
       "  [' glasses', ' regular', ' more'],\n",
       "  [' at', ' that', ' for'],\n",
       "  [' the', ' full', ' regular'],\n",
       "  [' regular', ' full', ' price'],\n",
       "  [' price', ' priced', ' discounted'],\n",
       "  [' of', ',', '.'],\n",
       "  [' $', ' ', ' one'],\n",
       "  ['5', '6', '4'],\n",
       "  [' each', ',', ' per'],\n",
       "  [',', '.', ' so'],\n",
       "  [' so', ' for', ' which'],\n",
       "  [' he', ' K', ' that'],\n",
       "  [' will', \"'ll\", ' pays'],\n",
       "  [' pay', ' spend', ' have'],\n",
       "  [' ', ' $', ' for'],\n",
       "  ['8', '5', '24'],\n",
       "  [' *', ' x', '*'],\n",
       "  [' $', ' ', '$'],\n",
       "  ['5', '<<', '3'],\n",
       "  [' =', ' =$', '='],\n",
       "  [' $', ' ', ' <<'],\n",
       "  ['<<', '40', '<'],\n",
       "  ['8', '24', '5'],\n",
       "  ['*', ' *', '*$'],\n",
       "  ['5', '3', '4'],\n",
       "  ['=', ' =', '-'],\n",
       "  ['40', '400', '39'],\n",
       "  ['>>', ' >>', '>'],\n",
       "  ['40', ' ', '30'],\n",
       "  [' for', '.\\n', '\\n'],\n",
       "  [' those', ' these', ' the'],\n",
       "  [' glasses', ' regular', ' '],\n",
       "  ['.\\n', '\\n', '.\\n\\n'],\n",
       "  ['In', 'K', 'The'],\n",
       "  [' total', ' all', ' order'],\n",
       "  [',', ' K', ' he'],\n",
       "  [' K', ' he', ' the'],\n",
       "  ['yl', 'yla', 'yr'],\n",
       "  ['ar', 'ars', 'ara'],\n",
       "  [' will', ' needs', ' has'],\n",
       "  [' pay', ' need', ' have'],\n",
       "  [' $', ' ', ' for'],\n",
       "  ['24', '40', '3'],\n",
       "  [' +', '+$', ' ('],\n",
       "  [' $', ' ', '$'],\n",
       "  ['40', ' ', '<<'],\n",
       "  [' =', ' =$', '='],\n",
       "  [' $', ' ', ' <<'],\n",
       "  ['<<', '64', '<'],\n",
       "  ['24', '64', '8'],\n",
       "  ['+', ' +', '*'],\n",
       "  ['40', ' ', '30'],\n",
       "  ['=', ' =', '=\"'],\n",
       "  ['64', '064', '164'],\n",
       "  ['>>', '>', ' >>'],\n",
       "  ['64', '65', '60'],\n",
       "  [' for', '.\\n', '\\n'],\n",
       "  [' the', ' all', ' '],\n",
       "  [' glasses', ' ', ' glass'],\n",
       "  ['.\\n', ' he', '\\n'],\n",
       "  ['####', 'The', 'So'],\n",
       "  [' ', ' $', '64'],\n",
       "  ['64', ' ', '62'],\n",
       "  ['\\n\\n', ' \\n\\n', '<|eot_id|>']],\n",
       " 'topk_logits': tensor([[16.2031, 15.9688, 15.6562],\n",
       "         [19.9688, 19.6875, 16.8438],\n",
       "         [20.4688, 17.9531, 16.4219],\n",
       "         [22.8906, 22.6562, 21.3438],\n",
       "         [23.4844, 21.4688, 20.1406],\n",
       "         [22.5781, 20.6094, 20.0000],\n",
       "         [23.9219, 23.2188, 21.6406],\n",
       "         [23.0938, 16.8906, 16.7188],\n",
       "         [21.9062, 18.4219, 17.6719],\n",
       "         [20.3281, 19.6562, 17.8125],\n",
       "         [23.1250, 20.9375, 20.3906],\n",
       "         [22.1406, 20.0469, 18.1250],\n",
       "         [24.5781, 16.2344, 14.0547],\n",
       "         [21.7812, 18.9844, 18.8594],\n",
       "         [23.8594, 23.6719, 18.6250],\n",
       "         [26.8750, 18.0625, 17.8906],\n",
       "         [22.2812, 19.6094, 17.9219],\n",
       "         [23.0156, 22.4062, 18.1875],\n",
       "         [21.7031, 21.6406, 16.5156],\n",
       "         [24.9219, 16.7344, 16.5625],\n",
       "         [27.0156, 16.2344, 15.7031],\n",
       "         [30.0938, 17.6094, 15.8438],\n",
       "         [28.2188, 24.6094, 16.1094],\n",
       "         [29.4062, 18.1719, 18.0781],\n",
       "         [29.2031, 17.5469, 15.5000],\n",
       "         [29.5938, 18.0312, 15.3828],\n",
       "         [24.1875, 16.1094, 15.8516],\n",
       "         [23.3125, 13.6016, 12.9453],\n",
       "         [20.3125, 19.5156, 17.9531],\n",
       "         [19.1094, 18.4219, 18.1562],\n",
       "         [23.8125, 17.3438, 16.4375],\n",
       "         [27.6094, 19.1250, 16.9531],\n",
       "         [21.8594, 21.8438, 21.4844],\n",
       "         [24.5000, 20.9062, 20.8438],\n",
       "         [22.9062, 19.1719, 18.9844],\n",
       "         [24.7188, 24.5625, 20.9844],\n",
       "         [20.0469, 18.4062, 18.2031],\n",
       "         [21.7812, 20.9062, 20.3594],\n",
       "         [23.1406, 21.3125, 21.1719],\n",
       "         [21.4062, 20.8594, 20.4531],\n",
       "         [25.0000, 21.5469, 18.5938],\n",
       "         [21.5312, 21.4219, 21.1875],\n",
       "         [23.5156, 21.6719, 17.4062],\n",
       "         [24.9219, 19.3438, 16.2969],\n",
       "         [20.0625, 19.1250, 18.9531],\n",
       "         [21.7656, 20.7344, 20.6719],\n",
       "         [21.2500, 21.0625, 21.0312],\n",
       "         [22.2188, 20.9688, 20.6719],\n",
       "         [24.4531, 22.2031, 21.6094],\n",
       "         [25.7656, 24.1719, 21.5469],\n",
       "         [23.9062, 20.6406, 20.3125],\n",
       "         [26.0000, 22.7969, 18.6562],\n",
       "         [22.3125, 20.6875, 20.4531],\n",
       "         [24.6562, 24.2969, 18.5000],\n",
       "         [26.1094, 19.2812, 17.6719],\n",
       "         [24.2812, 19.8281, 18.7812],\n",
       "         [25.0156, 20.3906, 16.9219],\n",
       "         [26.1406, 23.1250, 17.5312],\n",
       "         [27.9688, 18.3750, 17.3281],\n",
       "         [31.5000, 19.4844, 17.9219],\n",
       "         [34.4062, 16.8594, 15.4453],\n",
       "         [31.3594, 16.5625, 15.1406],\n",
       "         [36.7812, 15.8047, 15.5781],\n",
       "         [27.5469, 17.5156, 16.6875],\n",
       "         [28.1094, 14.0156, 13.6875],\n",
       "         [22.7812, 22.5625, 21.1719],\n",
       "         [23.5469, 22.7969, 22.4062],\n",
       "         [22.9688, 21.4688, 19.6094],\n",
       "         [24.9375, 20.2812, 17.5938],\n",
       "         [22.5625, 21.2969, 19.4531],\n",
       "         [24.8750, 16.8906, 16.1719],\n",
       "         [31.4688, 18.8281, 17.5469],\n",
       "         [25.3125, 21.4844, 21.2031],\n",
       "         [25.9844, 24.5469, 20.8438],\n",
       "         [25.4062, 22.6094, 21.5781],\n",
       "         [27.7031, 24.5312, 20.7031],\n",
       "         [23.0625, 19.5938, 18.5781],\n",
       "         [24.7188, 20.2500, 19.3438],\n",
       "         [25.6406, 22.2344, 21.3594],\n",
       "         [23.1875, 21.1719, 20.5156],\n",
       "         [24.5781, 17.6875, 17.4375],\n",
       "         [25.3125, 22.2812, 18.7812],\n",
       "         [25.5625, 21.3594, 18.2188],\n",
       "         [26.5312, 17.5312, 16.8594],\n",
       "         [24.1406, 21.2031, 18.7188],\n",
       "         [24.8594, 20.7031, 19.5781],\n",
       "         [26.4688, 20.6250, 20.4062],\n",
       "         [25.4688, 19.4688, 18.9531],\n",
       "         [26.7812, 19.8750, 18.5156],\n",
       "         [26.6250, 20.6406, 19.4062],\n",
       "         [25.4531, 20.0156, 19.7656],\n",
       "         [26.1250, 16.9688, 16.7188],\n",
       "         [25.7031, 20.6250, 18.1250],\n",
       "         [26.0312, 23.0312, 19.2656],\n",
       "         [26.5000, 17.4219, 17.4062],\n",
       "         [24.7188, 19.5312, 19.0469],\n",
       "         [25.7344, 18.5781, 15.1484],\n",
       "         [26.7812, 21.0781, 17.7344],\n",
       "         [29.2812, 18.9062, 18.0625],\n",
       "         [32.4375, 19.7188, 17.4844],\n",
       "         [31.9062, 17.2188, 16.6719],\n",
       "         [32.1875, 17.8438, 16.2812],\n",
       "         [36.0938, 16.1875, 15.5469],\n",
       "         [29.9219, 19.6562, 16.2344],\n",
       "         [27.5469, 14.9453, 14.1016],\n",
       "         [24.5625, 19.1719, 18.1094],\n",
       "         [25.8281, 21.3438, 20.3750],\n",
       "         [24.9688, 19.4531, 18.8594],\n",
       "         [25.9375, 19.7344, 17.4375],\n",
       "         [22.8750, 22.3750, 22.2188],\n",
       "         [25.4688, 20.2031, 17.9375],\n",
       "         [23.8281, 21.8906, 18.6406],\n",
       "         [24.4844, 21.7812, 21.0156],\n",
       "         [26.1719, 15.8203, 15.7109],\n",
       "         [33.0938, 22.5000, 19.4062],\n",
       "         [25.4688, 24.8750, 22.0156],\n",
       "         [26.8906, 25.8281, 25.1094],\n",
       "         [24.1875, 22.3438, 19.8906],\n",
       "         [25.1094, 21.3750, 18.4531],\n",
       "         [25.0938, 20.4219, 16.8594],\n",
       "         [26.7188, 21.4688, 20.8438],\n",
       "         [26.5469, 16.8750, 16.1875],\n",
       "         [25.2344, 20.3750, 18.5469],\n",
       "         [25.6562, 18.1406, 16.5000],\n",
       "         [25.7344, 22.5312, 17.9219],\n",
       "         [30.5156, 17.1719, 15.0625],\n",
       "         [34.5625, 20.2812, 18.6406],\n",
       "         [34.4688, 15.4922, 15.3047],\n",
       "         [31.9219, 16.8438, 14.7188],\n",
       "         [36.0000, 18.3750, 17.8438],\n",
       "         [29.3906, 18.0781, 17.7344],\n",
       "         [27.1875, 12.9766, 12.5078],\n",
       "         [23.5000, 21.9219, 20.6094],\n",
       "         [26.0938, 24.2812, 23.2031],\n",
       "         [25.2656, 22.4062, 19.2188],\n",
       "         [23.8906, 23.7344, 19.4219],\n",
       "         [21.7031, 15.6016, 15.1562],\n",
       "         [23.0625, 16.6406, 13.9766],\n",
       "         [24.8281, 15.2188, 13.5469],\n",
       "         [21.6094, 18.1094, 16.9531]]),\n",
       " 'topk_probabilities': tensor([[1.9568e-01, 1.5479e-01, 1.1325e-01],\n",
       "         [5.3501e-01, 4.0385e-01, 2.3507e-02],\n",
       "         [8.3145e-01, 6.7192e-02, 1.4531e-02],\n",
       "         [4.4623e-01, 3.5300e-01, 9.5009e-02],\n",
       "         [8.1219e-01, 1.0821e-01, 2.8674e-02],\n",
       "         [8.0228e-01, 1.1202e-01, 6.0906e-02],\n",
       "         [5.6397e-01, 2.7918e-01, 5.7613e-02],\n",
       "         [9.8708e-01, 1.9970e-03, 1.6816e-03],\n",
       "         [8.8841e-01, 2.7250e-02, 1.2872e-02],\n",
       "         [5.5361e-01, 2.8275e-01, 4.4738e-02],\n",
       "         [8.0908e-01, 9.0776e-02, 5.2537e-02],\n",
       "         [8.7027e-01, 1.0724e-01, 1.5692e-02],\n",
       "         [9.9958e-01, 2.3778e-04, 2.6887e-05],\n",
       "         [8.6603e-01, 5.2828e-02, 4.6621e-02],\n",
       "         [5.4317e-01, 4.5031e-01, 2.8952e-03],\n",
       "         [9.9887e-01, 1.4869e-04, 1.2521e-04],\n",
       "         [8.9855e-01, 6.2110e-02, 1.1489e-02],\n",
       "         [6.4309e-01, 3.4964e-01, 5.1457e-03],\n",
       "         [5.1135e-01, 4.8037e-01, 2.8564e-03],\n",
       "         [9.9935e-01, 2.7793e-04, 2.3404e-04],\n",
       "         [9.9994e-01, 2.0784e-05, 1.2218e-05],\n",
       "         [9.9999e-01, 3.7853e-06, 6.4759e-07],\n",
       "         [9.7362e-01, 2.6355e-02, 5.3624e-06],\n",
       "         [9.9997e-01, 1.3212e-05, 1.2029e-05],\n",
       "         [9.9999e-01, 8.6646e-06, 1.1189e-06],\n",
       "         [9.9999e-01, 9.5162e-06, 6.7339e-07],\n",
       "         [9.9859e-01, 3.0982e-04, 2.3941e-04],\n",
       "         [9.9949e-01, 6.0586e-05, 3.1432e-05],\n",
       "         [5.7461e-01, 2.5900e-01, 5.4289e-02],\n",
       "         [3.4972e-01, 1.7585e-01, 1.3483e-01],\n",
       "         [9.9668e-01, 1.5460e-03, 6.2465e-04],\n",
       "         [9.9971e-01, 2.0661e-04, 2.3546e-05],\n",
       "         [2.8961e-01, 2.8512e-01, 1.9904e-01],\n",
       "         [8.7322e-01, 2.4009e-02, 2.2555e-02],\n",
       "         [9.1191e-01, 2.1784e-02, 1.8059e-02],\n",
       "         [5.2766e-01, 4.5133e-01, 1.2605e-02],\n",
       "         [6.5875e-01, 1.2770e-01, 1.0423e-01],\n",
       "         [4.9200e-01, 2.0510e-01, 1.1870e-01],\n",
       "         [6.1357e-01, 9.8609e-02, 8.5673e-02],\n",
       "         [3.3995e-01, 1.9675e-01, 1.3106e-01],\n",
       "         [9.6486e-01, 3.0535e-02, 1.5932e-03],\n",
       "         [3.1312e-01, 2.8068e-01, 2.2204e-01],\n",
       "         [8.5622e-01, 1.3547e-01, 1.9025e-03],\n",
       "         [9.9565e-01, 3.7632e-03, 1.7878e-04],\n",
       "         [4.3204e-01, 1.6919e-01, 1.4247e-01],\n",
       "         [4.7990e-01, 1.7112e-01, 1.6075e-01],\n",
       "         [3.0262e-01, 2.5088e-01, 2.4316e-01],\n",
       "         [5.2347e-01, 1.4998e-01, 1.1145e-01],\n",
       "         [8.2215e-01, 8.6654e-02, 4.7855e-02],\n",
       "         [8.0858e-01, 1.6427e-01, 1.1900e-02],\n",
       "         [9.3247e-01, 3.5595e-02, 2.5638e-02],\n",
       "         [9.5926e-01, 3.8980e-02, 6.2028e-04],\n",
       "         [6.9518e-01, 1.3689e-01, 1.0829e-01],\n",
       "         [5.8735e-01, 4.1003e-01, 1.2453e-03],\n",
       "         [9.9856e-01, 1.0813e-03, 2.1628e-04],\n",
       "         [9.8226e-01, 1.1436e-02, 4.0143e-03],\n",
       "         [9.8959e-01, 9.7016e-03, 3.0226e-04],\n",
       "         [9.5281e-01, 4.6702e-02, 1.7378e-04],\n",
       "         [9.9990e-01, 6.8147e-05, 2.3922e-05],\n",
       "         [9.9999e-01, 6.0489e-06, 1.2679e-06],\n",
       "         [1.0000e+00, 2.3960e-08, 5.8260e-09],\n",
       "         [1.0000e+00, 3.7480e-07, 9.0424e-08],\n",
       "         [1.0000e+00, 7.7624e-10, 6.1887e-10],\n",
       "         [9.9991e-01, 4.3999e-05, 1.9222e-05],\n",
       "         [1.0000e+00, 7.5711e-07, 5.4533e-07],\n",
       "         [4.8891e-01, 3.9285e-01, 9.7787e-02],\n",
       "         [4.9782e-01, 2.3515e-01, 1.5911e-01],\n",
       "         [7.8493e-01, 1.7514e-01, 2.7282e-02],\n",
       "         [9.8795e-01, 9.3876e-03, 6.3883e-04],\n",
       "         [6.8075e-01, 1.9202e-01, 3.0381e-02],\n",
       "         [9.9923e-01, 3.4048e-04, 1.6594e-04],\n",
       "         [9.9999e-01, 3.2378e-06, 8.9909e-07],\n",
       "         [9.3391e-01, 2.0313e-02, 1.5333e-02],\n",
       "         [7.9762e-01, 1.8945e-01, 4.6693e-03],\n",
       "         [9.1977e-01, 5.6106e-02, 2.0005e-02],\n",
       "         [9.5783e-01, 4.0157e-02, 8.7343e-04],\n",
       "         [9.4349e-01, 2.9395e-02, 1.0646e-02],\n",
       "         [9.7445e-01, 1.1169e-02, 4.5126e-03],\n",
       "         [9.4395e-01, 3.1306e-02, 1.3050e-02],\n",
       "         [7.2725e-01, 9.6897e-02, 5.0269e-02],\n",
       "         [9.9459e-01, 1.0118e-03, 7.8797e-04],\n",
       "         [9.5020e-01, 4.5852e-02, 1.3846e-03],\n",
       "         [9.8366e-01, 1.4705e-02, 6.3605e-04],\n",
       "         [9.9962e-01, 1.2336e-04, 6.3008e-05],\n",
       "         [9.4127e-01, 4.9886e-02, 4.1593e-03],\n",
       "         [9.7397e-01, 1.5258e-02, 4.9537e-03],\n",
       "         [9.9135e-01, 2.8729e-03, 2.3084e-03],\n",
       "         [9.9266e-01, 2.4606e-03, 1.4693e-03],\n",
       "         [9.9830e-01, 9.9981e-04, 2.5677e-04],\n",
       "         [9.9526e-01, 2.5059e-03, 7.2925e-04],\n",
       "         [9.9157e-01, 4.3137e-03, 3.3595e-03],\n",
       "         [9.9935e-01, 1.0549e-04, 8.2156e-05],\n",
       "         [9.9242e-01, 6.1844e-03, 5.0764e-04],\n",
       "         [9.5134e-01, 4.7365e-02, 1.0966e-03],\n",
       "         [9.9959e-01, 1.1409e-04, 1.1232e-04],\n",
       "         [9.9034e-01, 5.5320e-03, 3.4082e-03],\n",
       "         [9.9909e-01, 7.7926e-04, 2.5246e-05],\n",
       "         [9.9636e-01, 3.3234e-03, 1.1733e-04],\n",
       "         [9.9995e-01, 3.1201e-05, 1.3420e-05],\n",
       "         [1.0000e+00, 2.9944e-06, 3.2058e-07],\n",
       "         [1.0000e+00, 4.1812e-07, 2.4199e-07],\n",
       "         [1.0000e+00, 5.8964e-07, 1.2360e-07],\n",
       "         [1.0000e+00, 2.2637e-09, 1.1929e-09],\n",
       "         [9.9996e-01, 3.4808e-05, 1.1365e-06],\n",
       "         [9.9999e-01, 3.3667e-06, 1.4480e-06],\n",
       "         [9.9195e-01, 4.5224e-03, 1.5629e-03],\n",
       "         [9.7897e-01, 1.1047e-02, 4.1928e-03],\n",
       "         [9.9110e-01, 3.9876e-03, 2.2022e-03],\n",
       "         [9.9699e-01, 2.0170e-03, 2.0286e-04],\n",
       "         [2.9051e-01, 1.7620e-01, 1.5071e-01],\n",
       "         [9.9212e-01, 5.1255e-03, 5.3184e-04],\n",
       "         [8.6795e-01, 1.2504e-01, 4.8483e-03],\n",
       "         [9.0292e-01, 6.0492e-02, 2.8131e-02],\n",
       "         [9.9992e-01, 3.1940e-05, 2.8631e-05],\n",
       "         [9.9997e-01, 2.5072e-05, 1.1365e-06],\n",
       "         [6.1141e-01, 3.3765e-01, 1.9349e-02],\n",
       "         [6.4932e-01, 2.2440e-01, 1.0936e-01],\n",
       "         [8.5048e-01, 1.3457e-01, 1.1576e-02],\n",
       "         [9.7262e-01, 2.3234e-02, 1.2507e-03],\n",
       "         [9.8942e-01, 9.2558e-03, 2.6257e-04],\n",
       "         [9.9194e-01, 5.2052e-03, 2.7862e-03],\n",
       "         [9.9977e-01, 6.3017e-05, 3.1687e-05],\n",
       "         [9.8984e-01, 7.6765e-03, 1.2337e-03],\n",
       "         [9.9922e-01, 5.4409e-04, 1.0548e-04],\n",
       "         [9.6034e-01, 3.9024e-02, 3.8860e-04],\n",
       "         [1.0000e+00, 1.6028e-06, 1.9444e-07],\n",
       "         [1.0000e+00, 6.2767e-07, 1.2168e-07],\n",
       "         [1.0000e+00, 5.7357e-09, 4.7550e-09],\n",
       "         [1.0000e+00, 2.8291e-07, 3.3789e-08],\n",
       "         [1.0000e+00, 2.2159e-08, 1.3027e-08],\n",
       "         [9.9997e-01, 1.2219e-05, 8.6645e-06],\n",
       "         [9.9999e-01, 6.7339e-07, 4.2140e-07],\n",
       "         [7.7109e-01, 1.5912e-01, 4.2828e-02],\n",
       "         [8.1221e-01, 1.3259e-01, 4.5111e-02],\n",
       "         [9.3969e-01, 5.3848e-02, 2.2226e-03],\n",
       "         [5.3273e-01, 4.5567e-01, 6.1060e-03],\n",
       "         [9.9220e-01, 2.2219e-03, 1.4234e-03],\n",
       "         [9.9809e-01, 1.6225e-03, 1.1303e-04],\n",
       "         [9.9977e-01, 6.7081e-05, 1.2604e-05],\n",
       "         [9.5598e-01, 2.8868e-02, 9.0837e-03]]),\n",
       " 'vocab_size': 128256}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(question_to_prompt(test_qnas[sample_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Sample Question: Kylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second glass costs only 60% of the price. Kylar wants to buy 16 glasses. How much does he need to pay for them?\n",
      "* Expected Answer: The discount price of one glass is 60/100 * 5 = $<<60/100*5=3>>3.\n",
      "If every second glass is cheaper, that means Kylar is going to buy 16 / 2 = <<16/2=8>>8 cheaper glasses.\n",
      "So for the cheaper glasses, Kylar is going to pay 8 * 3 = $<<8*3=24>>24.\n",
      "And for the regular-priced glasses, Kylar will pay 8 * 5 = $<<8*5=40>>40.\n",
      "So in total Kylar needs to pay 24 + 40 = $<<24+40=64>>64 for the glasses he wants to buy.\n",
      "#### 64\n"
     ]
    }
   ],
   "source": [
    "print(\"* Sample Question:\", test_qnas[sample_i]['question'])\n",
    "print(\"* Expected Answer:\", test_qnas[sample_i]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
