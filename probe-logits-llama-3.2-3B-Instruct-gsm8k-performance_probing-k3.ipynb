{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import set_seed\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sewoo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:777: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e5c9bbeff164722be3a65156a234683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s],))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 3072)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
      ") LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "# model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "# model_name = \"meta-llama/Llama-3.2-3B\"\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "# model_name = \"google/gemma-2-2b\"\n",
    "# model_name = \"google/gemma-2-2b-it\"\n",
    "# model_name = \"google/gemma-2-9b\"\n",
    "# model_name = \"google/gemma-2-9b-it\"\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    output_hidden_states=True,  # Enable hidden states\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "print(model, model.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7473, 1319)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.util.json_io import *\n",
    "\n",
    "train_qnas = load_jsonlines(f'data/gsm8k/train.jsonl')\n",
    "test_qnas = load_jsonlines(f'data/gsm8k/test.jsonl')\n",
    "len(train_qnas), len(test_qnas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: For every 12 cans you recycle, you receive $0.50, and for every 5 kilograms of newspapers, you receive $1.50. If your family collected 144 cans and 20 kilograms of newspapers, how much money would you receive?\n",
      "Answer: There are 144/12 = <<144/12=12>>12 sets of 12 cans that the family collected.\n",
      "So, the family would receive $0.50 x 12 = $<<0.50*12=6>>6 for the cans.\n",
      "There are 20/5 = <<20/5=4>>4 sets of 5 kilograms of newspapers that the family collected.\n",
      "So, the family would receive $1.50 x 4 = $<<1.50*4=6>>6 for the newspapers.\n",
      "Therefore, the family would receive a total of $6 + $6 = $<<6+6=12>>12.\n",
      "#### 12\n",
      "\n",
      "Question: Betty picked 16 strawberries. Matthew picked 20 more strawberries than Betty and twice as many as Natalie. They used their strawberries to make jam. One jar of jam used 7 strawberries and they sold each jar at $4. How much money were they able to make from the strawberries they picked?\n",
      "Answer: Matthew picked 16 + 20 = <<16+20=36>>36 strawberries.\n",
      "Natalie picked 36/2 = <<36/2=18>>18 strawberries.\n",
      "All together, they have 16 + 36 + 18 = <<16+36+18=70>>70 strawberries.\n",
      "They can make 70/7 = <<70/7=10>>10 jars of strawberries.\n",
      "They earn 10 x $4 = $<<10*4=40>>40 from the strawberries they picked.\n",
      "#### 40\n",
      "\n",
      "Question: Jack has a stack of books that is 12 inches thick. He knows from experience that 80 pages is one inch thick. If he has 6 books, how many pages is each one on average?\n",
      "Answer: There are 960 pages because 80 x 12 = <<80*12=960>>960\n",
      "Each book is 160 pages because 960 / 6 = <<960/6=160>>160\n",
      "#### 160\n",
      "\n",
      "Question: James dumps his whole collection of 500 Legos on the floor and starts building a castle out of them.  He uses half the pieces before finishing and is told to put the rest away.  He puts all of the leftover pieces back in the box they came from, except for 5 missing pieces that he can't find.  How many Legos are in the box at the end?\n",
      "Answer: James starts with 500 Legos and uses half of them, leaving 500/2=<<500/2=250>>250 Legos unused.\n",
      "He puts those unused Legos away but since he's missing 5 he only puts 250-5=<<250-5=245>>245 Legos away.\n",
      "#### 245\n",
      "\n",
      "Question: Ines had $20 in her purse. She bought 3 pounds of peaches, which are $2 per pound at the local farmers’ market. How much did she have left?\n",
      "Answer: Ines bought 3 pounds of peaches for 3 peaches * $2/peach = $<<3*2=6>>6.\n",
      "Ines has $20 - $6 = $<<20-6=14>>14 left.\n",
      "#### 14\n",
      "\n",
      "Question: Aaron pays his actuary membership fees each year. The membership fee increases yearly by $10. If he pays $80 in the first year, how much does his membership cost, in dollars, in the sixth year?\n",
      "Answer: In year 2 he pays 80+10=$<<80+10=90>>90.\n",
      "In year 3 he pays 90+10=$<<90+10=100>>100.\n",
      "In year 4 he pays 100+10=$<<100+10=110>>110.\n",
      "In year 5 he pays 110+10=$<<110+10=120>>120.\n",
      "In year 6 he pays 120+10=$<<120+10=130>>130.\n",
      "#### 130\n",
      "\n",
      "Question: Joseph invested $1000 into a hedge fund. The fund promised a yearly interest rate of 10%. If he deposited an additional $100 every month into the account to add to his initial investment of $1000, how much money will he have in the fund after two years?\n",
      "Answer: For the first year, Joseph will have invested $1000 + ($100 * 12) = $<<1000+100*12=2200>>2200.\n",
      "The interest calculated for the first year will be $2200 * 10% = $<<2200*10*.01=220>>220.\n",
      "The total value of the investment for the first year will be $2200 + $220 = $<<2200+220=2420>>2420.\n",
      "For the second year, the total invested will be $2420 + ($100 * 12) = $<<2420+100*12=3620>>3620.\n",
      "The interest calculated after the second year will be $3620 * 10% = $<<3620*10*.01=362>>362.\n",
      "Therefore, Joseph's investment in the mutual fund will be worth $3620 + $362 = $<<3620+362=3982>>3982.\n",
      "#### 3982\n",
      "\n",
      "Question: The price of buying a wooden toy at the new Craftee And Best store is $20, and the cost of buying a hat is $10. If Kendra went to the shop with a $100 bill and bought two wooden toys and three hats, calculate the change she received.\n",
      "Answer: When Kendra bought 2 toys, she paid 2*$20 = $<<2*20=40>>40\n",
      "Since the price of a hat is $10, when Kendra bought 3 hats, she paid 3*$10 = $<<3*10=30>>30\n",
      "The total costs for the hats and wooden toys Kendra bought is $40+$30 = $<<40+30=70>>70\n",
      "From the $100 bill, Kendra received change worth $100-$70 =$<<100-70=30>>30\n",
      "#### 30\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random; rseed = 42; random.seed(rseed)\n",
    "\n",
    "nshot_prompt = f\"\"\n",
    "for top_logit_indices in random.sample(range(len(train_qnas)), 8):\n",
    "    nshot_prompt += f\"Question: {train_qnas[top_logit_indices]['question']}\\nAnswer: {train_qnas[top_logit_indices]['answer']}\\n\\n\"\n",
    "\n",
    "print(nshot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: For every 12 cans you recycle, you receive $0.50, and for every 5 kilograms of newspapers, you receive $1.50. If your family collected 144 cans and 20 kilograms of newspapers, how much money would you receive?\n",
      "Answer: There are 144/12 = <<144/12=12>>12 sets of 12 cans that the family collected.\n",
      "So, the family would receive $0.50 x 12 = $<<0.50*12=6>>6 for the cans.\n",
      "There are 20/5 = <<20/5=4>>4 sets of 5 kilograms of newspapers that the family collected.\n",
      "So, the family would receive $1.50 x 4 = $<<1.50*4=6>>6 for the newspapers.\n",
      "Therefore, the family would receive a total of $6 + $6 = $<<6+6=12>>12.\n",
      "#### 12\n",
      "\n",
      "Question: Betty picked 16 strawberries. Matthew picked 20 more strawberries than Betty and twice as many as Natalie. They used their strawberries to make jam. One jar of jam used 7 strawberries and they sold each jar at $4. How much money were they able to make from the strawberries they picked?\n",
      "Answer: Matthew picked 16 + 20 = <<16+20=36>>36 strawberries.\n",
      "Natalie picked 36/2 = <<36/2=18>>18 strawberries.\n",
      "All together, they have 16 + 36 + 18 = <<16+36+18=70>>70 strawberries.\n",
      "They can make 70/7 = <<70/7=10>>10 jars of strawberries.\n",
      "They earn 10 x $4 = $<<10*4=40>>40 from the strawberries they picked.\n",
      "#### 40\n",
      "\n",
      "Question: Jack has a stack of books that is 12 inches thick. He knows from experience that 80 pages is one inch thick. If he has 6 books, how many pages is each one on average?\n",
      "Answer: There are 960 pages because 80 x 12 = <<80*12=960>>960\n",
      "Each book is 160 pages because 960 / 6 = <<960/6=160>>160\n",
      "#### 160\n",
      "\n",
      "Question: James dumps his whole collection of 500 Legos on the floor and starts building a castle out of them.  He uses half the pieces before finishing and is told to put the rest away.  He puts all of the leftover pieces back in the box they came from, except for 5 missing pieces that he can't find.  How many Legos are in the box at the end?\n",
      "Answer: James starts with 500 Legos and uses half of them, leaving 500/2=<<500/2=250>>250 Legos unused.\n",
      "He puts those unused Legos away but since he's missing 5 he only puts 250-5=<<250-5=245>>245 Legos away.\n",
      "#### 245\n",
      "\n",
      "Question: Ines had $20 in her purse. She bought 3 pounds of peaches, which are $2 per pound at the local farmers’ market. How much did she have left?\n",
      "Answer: Ines bought 3 pounds of peaches for 3 peaches * $2/peach = $<<3*2=6>>6.\n",
      "Ines has $20 - $6 = $<<20-6=14>>14 left.\n",
      "#### 14\n",
      "\n",
      "Question: Aaron pays his actuary membership fees each year. The membership fee increases yearly by $10. If he pays $80 in the first year, how much does his membership cost, in dollars, in the sixth year?\n",
      "Answer: In year 2 he pays 80+10=$<<80+10=90>>90.\n",
      "In year 3 he pays 90+10=$<<90+10=100>>100.\n",
      "In year 4 he pays 100+10=$<<100+10=110>>110.\n",
      "In year 5 he pays 110+10=$<<110+10=120>>120.\n",
      "In year 6 he pays 120+10=$<<120+10=130>>130.\n",
      "#### 130\n",
      "\n",
      "Question: Joseph invested $1000 into a hedge fund. The fund promised a yearly interest rate of 10%. If he deposited an additional $100 every month into the account to add to his initial investment of $1000, how much money will he have in the fund after two years?\n",
      "Answer: For the first year, Joseph will have invested $1000 + ($100 * 12) = $<<1000+100*12=2200>>2200.\n",
      "The interest calculated for the first year will be $2200 * 10% = $<<2200*10*.01=220>>220.\n",
      "The total value of the investment for the first year will be $2200 + $220 = $<<2200+220=2420>>2420.\n",
      "For the second year, the total invested will be $2420 + ($100 * 12) = $<<2420+100*12=3620>>3620.\n",
      "The interest calculated after the second year will be $3620 * 10% = $<<3620*10*.01=362>>362.\n",
      "Therefore, Joseph's investment in the mutual fund will be worth $3620 + $362 = $<<3620+362=3982>>3982.\n",
      "#### 3982\n",
      "\n",
      "Question: The price of buying a wooden toy at the new Craftee And Best store is $20, and the cost of buying a hat is $10. If Kendra went to the shop with a $100 bill and bought two wooden toys and three hats, calculate the change she received.\n",
      "Answer: When Kendra bought 2 toys, she paid 2*$20 = $<<2*20=40>>40\n",
      "Since the price of a hat is $10, when Kendra bought 3 hats, she paid 3*$10 = $<<3*10=30>>30\n",
      "The total costs for the hats and wooden toys Kendra bought is $40+$30 = $<<40+30=70>>70\n",
      "From the $100 bill, Kendra received change worth $100-$70 =$<<100-70=30>>30\n",
      "#### 30\n",
      "\n",
      "Question: John drives for 3 hours at a speed of 60 mph and then turns around because he realizes he forgot something very important at home.  He tries to get home in 4 hours but spends the first 2 hours in standstill traffic.  He spends the next half-hour driving at a speed of 30mph, before being able to drive the remaining time of the 4 hours going at 80 mph.  How far is he from home at the end of those 4 hours? Let's think step by step.\n",
      "Answer: \n",
      "Answer: 45\n",
      "Answer in integer: 45\n"
     ]
    }
   ],
   "source": [
    "def question_to_prompt(question):\n",
    "    return f\"{nshot_prompt}Question: {question} Let's think step by step.\\nAnswer: \"\n",
    "\n",
    "sample_i = 8\n",
    "print(question_to_prompt(test_qnas[sample_i]['question']))\n",
    "\n",
    "from src.util.gsm8k_helper import *\n",
    "print('Answer:', extract_num_from_ans(test_qnas[sample_i]['answer']))\n",
    "print('Answer in integer:', extract_num_from_ans(test_qnas[sample_i]['answer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(input_text, top_k=1):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_length=input_ids.shape[1] + 512,\n",
    "            do_sample=True, top_k=top_k,\n",
    "            eos_token_id=tokenizer.encode(text='\\n\\n', add_special_tokens=False)[0],\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            return_dict_in_generate=True, \n",
    "            output_logits=True, \n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "\n",
    "    output_text = tokenizer.decode(outputs.sequences[0])\n",
    "    generated_answer = output_text.split('Answer: ')[-1].split('\\n\\n')[0]\n",
    "    generated_len = len(outputs.logits)\n",
    "\n",
    "    k = 3\n",
    "    topk_indices = torch.zeros((generated_len, k), dtype=torch.long)\n",
    "    topk_logits = torch.zeros((generated_len, k))\n",
    "    topk_probabilities = torch.zeros((generated_len, k))\n",
    "\n",
    "    # Iterate over each sequence position to find the top-3 indices and their logits and probabilities\n",
    "    for seq_idx, logits_tensor in enumerate(outputs.logits): # outputs.logits: (seq_length, batch_size, vocab_size)\n",
    "        logits = logits_tensor[0]  # score_tensor.shape: (batch_size, vocab_size)\n",
    "        \n",
    "        top_logit_values, top_logit_indices = torch.topk(logits, k=3)\n",
    "        \n",
    "        topk_indices[seq_idx] = top_logit_indices  # Indices of the top-3 tokens\n",
    "        topk_logits[seq_idx] = top_logit_values  # Logits of the top-3 tokens\n",
    "        topk_probabilities[seq_idx] = torch.nn.functional.softmax(logits, dim=-1)[top_logit_indices]  # Probabilities of the top-3 tokens\n",
    "\n",
    "    return {\n",
    "        'generated_answer': generated_answer,\n",
    "        'generated_indices': outputs.sequences[0][input_ids.shape[1]:],\n",
    "        'generated_tokens': [tokenizer.decode(i) for i in outputs.sequences[0][input_ids.shape[1]:]],\n",
    "        'generated_token_len': len(outputs.sequences[0][input_ids.shape[1]:]),\n",
    "        'topk_indices': topk_indices,\n",
    "        'topk_tokens': [[tokenizer.decode(i) for i in row] for row in topk_indices],\n",
    "        'topk_logits': topk_logits,\n",
    "        'topk_probabilities': topk_probabilities,\n",
    "        'vocab_size': outputs.logits[0].shape[-1],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1319 [00:00<?, ?it/s]c:\\Users\\sewoo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:655: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n",
      "100%|██████████| 1319/1319 [25:15:25<00:00, 68.93s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 1319 test samples.\n",
      "Accuracy: 0.6103108415466262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Collect features and labels from test data\n",
    "test_features = []\n",
    "test_labels = []\n",
    "\n",
    "print(\"Processing test data...\")\n",
    "for i, qna in enumerate(tqdm(test_qnas[:])): # Change here (e.g., qnas[:20]) for quick testing\n",
    "\n",
    "    ans_data = generate_answer(question_to_prompt(qna['question']), top_k=3)\n",
    "\n",
    "    generated_answer_int = extract_num_from_ans(ans_data['generated_answer'])\n",
    "    ground_truth_int = extract_num_from_ans(qna['answer'])\n",
    "\n",
    "    label = int(generated_answer_int == ground_truth_int)\n",
    "\n",
    "    test_features.append(ans_data)\n",
    "    test_labels.append(label)\n",
    "\n",
    "print(f\"Collected {len(test_features)} test samples.\")\n",
    "print(f\"Accuracy: {np.mean(test_labels)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
