{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sewoo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:777: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "701e9b1eba4b468384fd607645dc2293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s],))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 3072)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
      ") LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "# model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "# model_name = \"meta-llama/Llama-3.2-3B\"\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "# model_name = \"google/gemma-2-2b\"\n",
    "# model_name = \"google/gemma-2-2b-it\"\n",
    "# model_name = \"google/gemma-2-9b\"\n",
    "# model_name = \"google/gemma-2-9b-it\"\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    output_hidden_states=True,  # Enable hidden states\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "print(model, model.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7473, 1319)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.util.json_io import *\n",
    "\n",
    "train_qnas = load_jsonlines(f'data/gsm8k/train.jsonl')\n",
    "test_qnas = load_jsonlines(f'data/gsm8k/test.jsonl')\n",
    "len(train_qnas), len(test_qnas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: For every 12 cans you recycle, you receive $0.50, and for every 5 kilograms of newspapers, you receive $1.50. If your family collected 144 cans and 20 kilograms of newspapers, how much money would you receive?\n",
      "Answer: There are 144/12 = <<144/12=12>>12 sets of 12 cans that the family collected.\n",
      "So, the family would receive $0.50 x 12 = $<<0.50*12=6>>6 for the cans.\n",
      "There are 20/5 = <<20/5=4>>4 sets of 5 kilograms of newspapers that the family collected.\n",
      "So, the family would receive $1.50 x 4 = $<<1.50*4=6>>6 for the newspapers.\n",
      "Therefore, the family would receive a total of $6 + $6 = $<<6+6=12>>12.\n",
      "#### 12\n",
      "\n",
      "Question: Betty picked 16 strawberries. Matthew picked 20 more strawberries than Betty and twice as many as Natalie. They used their strawberries to make jam. One jar of jam used 7 strawberries and they sold each jar at $4. How much money were they able to make from the strawberries they picked?\n",
      "Answer: Matthew picked 16 + 20 = <<16+20=36>>36 strawberries.\n",
      "Natalie picked 36/2 = <<36/2=18>>18 strawberries.\n",
      "All together, they have 16 + 36 + 18 = <<16+36+18=70>>70 strawberries.\n",
      "They can make 70/7 = <<70/7=10>>10 jars of strawberries.\n",
      "They earn 10 x $4 = $<<10*4=40>>40 from the strawberries they picked.\n",
      "#### 40\n",
      "\n",
      "Question: Jack has a stack of books that is 12 inches thick. He knows from experience that 80 pages is one inch thick. If he has 6 books, how many pages is each one on average?\n",
      "Answer: There are 960 pages because 80 x 12 = <<80*12=960>>960\n",
      "Each book is 160 pages because 960 / 6 = <<960/6=160>>160\n",
      "#### 160\n",
      "\n",
      "Question: James dumps his whole collection of 500 Legos on the floor and starts building a castle out of them.  He uses half the pieces before finishing and is told to put the rest away.  He puts all of the leftover pieces back in the box they came from, except for 5 missing pieces that he can't find.  How many Legos are in the box at the end?\n",
      "Answer: James starts with 500 Legos and uses half of them, leaving 500/2=<<500/2=250>>250 Legos unused.\n",
      "He puts those unused Legos away but since he's missing 5 he only puts 250-5=<<250-5=245>>245 Legos away.\n",
      "#### 245\n",
      "\n",
      "Question: Ines had $20 in her purse. She bought 3 pounds of peaches, which are $2 per pound at the local farmersâ€™ market. How much did she have left?\n",
      "Answer: Ines bought 3 pounds of peaches for 3 peaches * $2/peach = $<<3*2=6>>6.\n",
      "Ines has $20 - $6 = $<<20-6=14>>14 left.\n",
      "#### 14\n",
      "\n",
      "Question: Aaron pays his actuary membership fees each year. The membership fee increases yearly by $10. If he pays $80 in the first year, how much does his membership cost, in dollars, in the sixth year?\n",
      "Answer: In year 2 he pays 80+10=$<<80+10=90>>90.\n",
      "In year 3 he pays 90+10=$<<90+10=100>>100.\n",
      "In year 4 he pays 100+10=$<<100+10=110>>110.\n",
      "In year 5 he pays 110+10=$<<110+10=120>>120.\n",
      "In year 6 he pays 120+10=$<<120+10=130>>130.\n",
      "#### 130\n",
      "\n",
      "Question: Joseph invested $1000 into a hedge fund. The fund promised a yearly interest rate of 10%. If he deposited an additional $100 every month into the account to add to his initial investment of $1000, how much money will he have in the fund after two years?\n",
      "Answer: For the first year, Joseph will have invested $1000 + ($100 * 12) = $<<1000+100*12=2200>>2200.\n",
      "The interest calculated for the first year will be $2200 * 10% = $<<2200*10*.01=220>>220.\n",
      "The total value of the investment for the first year will be $2200 + $220 = $<<2200+220=2420>>2420.\n",
      "For the second year, the total invested will be $2420 + ($100 * 12) = $<<2420+100*12=3620>>3620.\n",
      "The interest calculated after the second year will be $3620 * 10% = $<<3620*10*.01=362>>362.\n",
      "Therefore, Joseph's investment in the mutual fund will be worth $3620 + $362 = $<<3620+362=3982>>3982.\n",
      "#### 3982\n",
      "\n",
      "Question: The price of buying a wooden toy at the new Craftee And Best store is $20, and the cost of buying a hat is $10. If Kendra went to the shop with a $100 bill and bought two wooden toys and three hats, calculate the change she received.\n",
      "Answer: When Kendra bought 2 toys, she paid 2*$20 = $<<2*20=40>>40\n",
      "Since the price of a hat is $10, when Kendra bought 3 hats, she paid 3*$10 = $<<3*10=30>>30\n",
      "The total costs for the hats and wooden toys Kendra bought is $40+$30 = $<<40+30=70>>70\n",
      "From the $100 bill, Kendra received change worth $100-$70 =$<<100-70=30>>30\n",
      "#### 30\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random; rseed = 42; random.seed(rseed)\n",
    "\n",
    "nshot_prompt = f\"\"\n",
    "for top_logit_indices in random.sample(range(len(train_qnas)), 8):\n",
    "    nshot_prompt += f\"Question: {train_qnas[top_logit_indices]['question']}\\nAnswer: {train_qnas[top_logit_indices]['answer']}\\n\\n\"\n",
    "\n",
    "print(nshot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: For every 12 cans you recycle, you receive $0.50, and for every 5 kilograms of newspapers, you receive $1.50. If your family collected 144 cans and 20 kilograms of newspapers, how much money would you receive?\n",
      "Answer: There are 144/12 = <<144/12=12>>12 sets of 12 cans that the family collected.\n",
      "So, the family would receive $0.50 x 12 = $<<0.50*12=6>>6 for the cans.\n",
      "There are 20/5 = <<20/5=4>>4 sets of 5 kilograms of newspapers that the family collected.\n",
      "So, the family would receive $1.50 x 4 = $<<1.50*4=6>>6 for the newspapers.\n",
      "Therefore, the family would receive a total of $6 + $6 = $<<6+6=12>>12.\n",
      "#### 12\n",
      "\n",
      "Question: Betty picked 16 strawberries. Matthew picked 20 more strawberries than Betty and twice as many as Natalie. They used their strawberries to make jam. One jar of jam used 7 strawberries and they sold each jar at $4. How much money were they able to make from the strawberries they picked?\n",
      "Answer: Matthew picked 16 + 20 = <<16+20=36>>36 strawberries.\n",
      "Natalie picked 36/2 = <<36/2=18>>18 strawberries.\n",
      "All together, they have 16 + 36 + 18 = <<16+36+18=70>>70 strawberries.\n",
      "They can make 70/7 = <<70/7=10>>10 jars of strawberries.\n",
      "They earn 10 x $4 = $<<10*4=40>>40 from the strawberries they picked.\n",
      "#### 40\n",
      "\n",
      "Question: Jack has a stack of books that is 12 inches thick. He knows from experience that 80 pages is one inch thick. If he has 6 books, how many pages is each one on average?\n",
      "Answer: There are 960 pages because 80 x 12 = <<80*12=960>>960\n",
      "Each book is 160 pages because 960 / 6 = <<960/6=160>>160\n",
      "#### 160\n",
      "\n",
      "Question: James dumps his whole collection of 500 Legos on the floor and starts building a castle out of them.  He uses half the pieces before finishing and is told to put the rest away.  He puts all of the leftover pieces back in the box they came from, except for 5 missing pieces that he can't find.  How many Legos are in the box at the end?\n",
      "Answer: James starts with 500 Legos and uses half of them, leaving 500/2=<<500/2=250>>250 Legos unused.\n",
      "He puts those unused Legos away but since he's missing 5 he only puts 250-5=<<250-5=245>>245 Legos away.\n",
      "#### 245\n",
      "\n",
      "Question: Ines had $20 in her purse. She bought 3 pounds of peaches, which are $2 per pound at the local farmersâ€™ market. How much did she have left?\n",
      "Answer: Ines bought 3 pounds of peaches for 3 peaches * $2/peach = $<<3*2=6>>6.\n",
      "Ines has $20 - $6 = $<<20-6=14>>14 left.\n",
      "#### 14\n",
      "\n",
      "Question: Aaron pays his actuary membership fees each year. The membership fee increases yearly by $10. If he pays $80 in the first year, how much does his membership cost, in dollars, in the sixth year?\n",
      "Answer: In year 2 he pays 80+10=$<<80+10=90>>90.\n",
      "In year 3 he pays 90+10=$<<90+10=100>>100.\n",
      "In year 4 he pays 100+10=$<<100+10=110>>110.\n",
      "In year 5 he pays 110+10=$<<110+10=120>>120.\n",
      "In year 6 he pays 120+10=$<<120+10=130>>130.\n",
      "#### 130\n",
      "\n",
      "Question: Joseph invested $1000 into a hedge fund. The fund promised a yearly interest rate of 10%. If he deposited an additional $100 every month into the account to add to his initial investment of $1000, how much money will he have in the fund after two years?\n",
      "Answer: For the first year, Joseph will have invested $1000 + ($100 * 12) = $<<1000+100*12=2200>>2200.\n",
      "The interest calculated for the first year will be $2200 * 10% = $<<2200*10*.01=220>>220.\n",
      "The total value of the investment for the first year will be $2200 + $220 = $<<2200+220=2420>>2420.\n",
      "For the second year, the total invested will be $2420 + ($100 * 12) = $<<2420+100*12=3620>>3620.\n",
      "The interest calculated after the second year will be $3620 * 10% = $<<3620*10*.01=362>>362.\n",
      "Therefore, Joseph's investment in the mutual fund will be worth $3620 + $362 = $<<3620+362=3982>>3982.\n",
      "#### 3982\n",
      "\n",
      "Question: The price of buying a wooden toy at the new Craftee And Best store is $20, and the cost of buying a hat is $10. If Kendra went to the shop with a $100 bill and bought two wooden toys and three hats, calculate the change she received.\n",
      "Answer: When Kendra bought 2 toys, she paid 2*$20 = $<<2*20=40>>40\n",
      "Since the price of a hat is $10, when Kendra bought 3 hats, she paid 3*$10 = $<<3*10=30>>30\n",
      "The total costs for the hats and wooden toys Kendra bought is $40+$30 = $<<40+30=70>>70\n",
      "From the $100 bill, Kendra received change worth $100-$70 =$<<100-70=30>>30\n",
      "#### 30\n",
      "\n",
      "Question: Kylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second glass costs only 60% of the price. Kylar wants to buy 16 glasses. How much does he need to pay for them? Let's think step by step.\n",
      "Answer: \n",
      "Answer: 64\n",
      "Answer in integer: 64\n"
     ]
    }
   ],
   "source": [
    "def question_to_prompt(question):\n",
    "    return f\"{nshot_prompt}Question: {question} Let's think step by step.\\nAnswer: \"\n",
    "\n",
    "sample_i = 5\n",
    "print(question_to_prompt(test_qnas[sample_i]['question']))\n",
    "\n",
    "from src.util.gsm8k_helper import *\n",
    "print('Answer:', extract_num_from_ans(test_qnas[sample_i]['answer']))\n",
    "print('Answer in integer:', extract_num_from_ans(test_qnas[sample_i]['answer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sewoo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:655: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs.logits[0].shape: torch.Size([1, 128256])\n",
      "generated_len = output length - prompt length = 100\n",
      "top3 indices: tensor([845,  23,  16])\n",
      "outputs.logits: tensor([19.3594, 17.1094, 16.8750])\n",
      "outputs.scores: tensor([193593.7500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([29247,    14,   374])\n",
      "outputs.logits: tensor([18.4219, 17.3281, 16.5156])\n",
      "outputs.scores: tensor([184218.7500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([ 374,  690, 2853])\n",
      "outputs.logits: tensor([16.8594, 16.0781, 15.9766])\n",
      "outputs.scores: tensor([168593.7500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([220, 459, 264])\n",
      "outputs.logits: tensor([19.7812, 19.2812, 18.1250])\n",
      "outputs.scores: tensor([197812.5000,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([ 23, 845,  17])\n",
      "outputs.logits: tensor([22.1094, 21.0781, 19.7031])\n",
      "outputs.scores: tensor([221093.7500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([13840, 29247,  7437])\n",
      "outputs.logits: tensor([18.2812, 17.7031, 17.7031])\n",
      "outputs.scores: tensor([182812.5000,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([315,  13,  11])\n",
      "outputs.logits: tensor([20.0000, 18.8281, 18.5156])\n",
      "outputs.scores: tensor([200000.,    -inf,    -inf], device='cuda:0')\n",
      "top3 indices: tensor([29247,   220,  1403])\n",
      "outputs.logits: tensor([23.5625, 18.2188, 17.0000])\n",
      "outputs.scores: tensor([235625.,    -inf,    -inf], device='cuda:0')\n",
      "top3 indices: tensor([ 13, 627,  11])\n",
      "outputs.logits: tensor([19.7812, 19.4219, 18.9531])\n",
      "outputs.scores: tensor([197812.5000,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([ 220, 8876, 9062])\n",
      "outputs.logits: tensor([18.2500, 18.0469, 17.8281])\n",
      "outputs.scores: tensor([182500.,    -inf,    -inf], device='cuda:0')\n",
      "top3 indices: tensor([  23, 3861,   16])\n",
      "outputs.logits: tensor([19.5781, 18.0000, 17.7969])\n",
      "outputs.scores: tensor([195781.2500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([29247, 13840,   865])\n",
      "outputs.logits: tensor([19.4062, 19.0625, 17.5781])\n",
      "outputs.scores: tensor([194062.5000,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([2853,  690,  527])\n",
      "outputs.logits: tensor([18.8281, 17.6250, 17.5781])\n",
      "outputs.scores: tensor([188281.2500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([220, 400, 279])\n",
      "outputs.logits: tensor([21.2656, 20.5938, 16.5156])\n",
      "outputs.scores: tensor([212656.2500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([23, 20, 17])\n",
      "outputs.logits: tensor([24.4844, 21.6562, 20.0156])\n",
      "outputs.scores: tensor([244843.7500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([  353, 40118,   865])\n",
      "outputs.logits: tensor([19.7031, 19.4688, 19.4531])\n",
      "outputs.scores: tensor([197031.2500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([400, 220,   3])\n",
      "outputs.logits: tensor([22.5469, 21.0781, 17.8281])\n",
      "outputs.scores: tensor([225468.7500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([  20,  220, 2501])\n",
      "outputs.logits: tensor([23.3750, 16.9375, 16.7812])\n",
      "outputs.scores: tensor([233750.,    -inf,    -inf], device='cuda:0')\n",
      "top3 indices: tensor([  284, 47481,    28])\n",
      "outputs.logits: tensor([22.1094, 18.8125, 18.3906])\n",
      "outputs.scores: tensor([221093.7500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([ 400,  220, 1134])\n",
      "outputs.logits: tensor([23.3594, 19.2812, 16.4531])\n",
      "outputs.scores: tensor([233593.7500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([2501, 1272,   27])\n",
      "outputs.logits: tensor([23.8906, 21.0312, 16.5938])\n",
      "outputs.scores: tensor([238906.2500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([ 23, 845,  20])\n",
      "outputs.logits: tensor([25.0156, 17.8594, 15.2969])\n",
      "outputs.scores: tensor([250156.2500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([    9,   353, 40118])\n",
      "outputs.logits: tensor([29.7656, 20.8281, 17.3438])\n",
      "outputs.scores: tensor([297656.2500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([ 20, 220,  19])\n",
      "outputs.logits: tensor([30.7344, 17.2344, 16.5938])\n",
      "outputs.scores: tensor([307343.7500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([ 28, 284,  10])\n",
      "outputs.logits: tensor([30.3750, 16.8438, 15.8047])\n",
      "outputs.scores: tensor([303750.,    -inf,    -inf], device='cuda:0')\n",
      "top3 indices: tensor([1272, 3443,   19])\n",
      "outputs.logits: tensor([34.6875, 15.5469, 14.5234])\n",
      "outputs.scores: tensor([346875.,    -inf,    -inf], device='cuda:0')\n",
      "top3 indices: tensor([ 2511,    29, 40171])\n",
      "outputs.logits: tensor([25.5000, 17.6406, 17.5938])\n",
      "outputs.scores: tensor([255000.,    -inf,    -inf], device='cuda:0')\n",
      "top3 indices: tensor([1272,  220,    3])\n",
      "outputs.logits: tensor([21.9062, 13.3516, 12.4141])\n",
      "outputs.scores: tensor([219062.5000,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([627, 198,  13])\n",
      "outputs.logits: tensor([20.6250, 20.5781, 17.5000])\n",
      "outputs.scores: tensor([206250.,    -inf,    -inf], device='cuda:0')\n",
      "top3 indices: tensor([  23,  791, 2520])\n",
      "outputs.logits: tensor([18.2500, 18.1875, 18.0625])\n",
      "outputs.scores: tensor([182500.,    -inf,    -inf], device='cuda:0')\n",
      "top3 indices: tensor([29247,   353,   611])\n",
      "outputs.logits: tensor([20.2812, 17.3125, 16.9844])\n",
      "outputs.scores: tensor([202812.5000,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([ 527,  374, 2853])\n",
      "outputs.logits: tensor([18.0000, 17.7031, 17.0625])\n",
      "outputs.scores: tensor([180000.,    -inf,    -inf], device='cuda:0')\n",
      "top3 indices: tensor([220, 279, 539])\n",
      "outputs.logits: tensor([18.0156, 17.4062, 17.1719])\n",
      "outputs.scores: tensor([180156.2500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([ 19,  23, 845])\n",
      "outputs.logits: tensor([22.2031, 20.9375, 19.4688])\n",
      "outputs.scores: tensor([222031.2500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([29247, 13840,  7437])\n",
      "outputs.logits: tensor([18.9531, 16.5156, 15.8906])\n",
      "outputs.scores: tensor([189531.2500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([430, 449, 369])\n",
      "outputs.logits: tensor([19.4219, 17.6875, 17.0000])\n",
      "outputs.scores: tensor([194218.7500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([2853,  527,  617])\n",
      "outputs.logits: tensor([21.5156, 20.8125, 18.1406])\n",
      "outputs.scores: tensor([215156.2500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([ 220,  400, 1193])\n",
      "outputs.logits: tensor([21.0938, 20.6094, 20.0156])\n",
      "outputs.scores: tensor([210937.5000,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([1399, 1135, 1041])\n",
      "outputs.logits: tensor([21.5312, 19.5781, 19.1875])\n",
      "outputs.scores: tensor([215312.5000,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([    4, 14697, 13689])\n",
      "outputs.logits: tensor([21.8281, 18.8125, 18.1875])\n",
      "outputs.scores: tensor([218281.2500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([ 315,  439, 2753])\n",
      "outputs.logits: tensor([22.2031, 17.7344, 17.6562])\n",
      "outputs.scores: tensor([222031.2500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([279, 400, 872])\n",
      "outputs.logits: tensor([23.5469, 22.5156, 21.0469])\n",
      "outputs.scores: tensor([235468.7500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([3430, 4113, 4725])\n",
      "outputs.logits: tensor([23.2812, 21.0156, 20.2812])\n",
      "outputs.scores: tensor([232812.5000,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([ 13, 315,  11])\n",
      "outputs.logits: tensor([20.6562, 20.4062, 19.8594])\n",
      "outputs.scores: tensor([206562.5000,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([ 220, 2100, 8876])\n",
      "outputs.logits: tensor([20.0938, 18.2812, 17.9844])\n",
      "outputs.scores: tensor([200937.5000,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([1399,   19,   16])\n",
      "outputs.logits: tensor([20.7500, 20.7500, 18.7188])\n",
      "outputs.scores: tensor([207500., 207500.,    -inf], device='cuda:0')\n",
      "top3 indices: tensor([29247,   353,   865])\n",
      "outputs.logits: tensor([21.3125, 19.7031, 16.8906])\n",
      "outputs.scores: tensor([213125.,    -inf,    -inf], device='cuda:0')\n",
      "top3 indices: tensor([2853,  527,  353])\n",
      "outputs.logits: tensor([21.5000, 19.7812, 18.6250])\n",
      "outputs.scores: tensor([215000.,    -inf,    -inf], device='cuda:0')\n",
      "top3 indices: tensor([220, 400, 320])\n",
      "outputs.logits: tensor([22.0938, 19.8750, 18.4688])\n",
      "outputs.scores: tensor([220937.5000,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([  19, 1399,   20])\n",
      "outputs.logits: tensor([24.7188, 21.1094, 19.7031])\n",
      "outputs.scores: tensor([247187.5000,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([ 353,    9, 6737])\n",
      "outputs.logits: tensor([22.1562, 17.7969, 17.5000])\n",
      "outputs.scores: tensor([221562.5000,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([220, 400, 320])\n",
      "outputs.logits: tensor([21.0312, 20.9688, 20.3750])\n",
      "outputs.scores: tensor([210312.5000,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([  15, 1399, 1135])\n",
      "outputs.logits: tensor([22.5156, 22.0781, 18.5625])\n",
      "outputs.scores: tensor([225156.2500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([ 13,  11, 662])\n",
      "outputs.logits: tensor([24.1094, 17.9062, 17.0781])\n",
      "outputs.scores: tensor([241093.7500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([  21, 1399,   20])\n",
      "outputs.logits: tensor([22.9531, 22.3125, 17.3438])\n",
      "outputs.scores: tensor([229531.2500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([  353,   284, 40118])\n",
      "outputs.logits: tensor([22.0312, 19.0938, 19.0312])\n",
      "outputs.scores: tensor([220312.5000,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([400, 220,   3])\n",
      "outputs.logits: tensor([23.8906, 20.8750, 19.4531])\n",
      "outputs.scores: tensor([238906.2500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([  20,  220, 2501])\n",
      "outputs.logits: tensor([25.0781, 17.5938, 16.7031])\n",
      "outputs.scores: tensor([250781.2500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([  284,    28, 47481])\n",
      "outputs.logits: tensor([22.5469, 18.7188, 18.6719])\n",
      "outputs.scores: tensor([225468.7500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([ 400,  220, 1134])\n",
      "outputs.logits: tensor([23.7656, 21.5625, 18.0781])\n",
      "outputs.scores: tensor([237656.2500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([ 2501,   717, 33584])\n",
      "outputs.logits: tensor([22.4531, 20.4844, 18.0781])\n",
      "outputs.scores: tensor([224531.2500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([  19,   23, 1272])\n",
      "outputs.logits: tensor([27.4531, 21.3281, 14.2656])\n",
      "outputs.scores: tensor([274531.2500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([    9, 20517,   353])\n",
      "outputs.logits: tensor([28.1250, 21.5938, 18.6562])\n",
      "outputs.scores: tensor([281250.,    -inf,    -inf], device='cuda:0')\n",
      "top3 indices: tensor([ 15, 662, 220])\n",
      "outputs.logits: tensor([26.6250, 17.8594, 17.4375])\n",
      "outputs.scores: tensor([266250.,    -inf,    -inf], device='cuda:0')\n",
      "top3 indices: tensor([   13,    11, 20517])\n",
      "outputs.logits: tensor([27.2500, 15.2969, 14.7344])\n",
      "outputs.scores: tensor([272500.,    -inf,    -inf], device='cuda:0')\n",
      "top3 indices: tensor([  21, 2705,   23])\n",
      "outputs.logits: tensor([29.4375, 17.2500, 16.3281])\n",
      "outputs.scores: tensor([294375.,    -inf,    -inf], device='cuda:0')\n",
      "top3 indices: tensor([    9, 20517, 40118])\n",
      "outputs.logits: tensor([28.8906, 17.8750, 17.4531])\n",
      "outputs.scores: tensor([288906.2500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([ 20, 220,  19])\n",
      "outputs.logits: tensor([31.2656, 18.9375, 16.4688])\n",
      "outputs.scores: tensor([312656.2500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([   28,   284, 47034])\n",
      "outputs.logits: tensor([29.9375, 17.5312, 14.7422])\n",
      "outputs.scores: tensor([299375.,    -inf,    -inf], device='cuda:0')\n",
      "top3 indices: tensor([  717, 70332,   806])\n",
      "outputs.logits: tensor([28.5625, 13.3906, 13.1172])\n",
      "outputs.scores: tensor([285625.,    -inf,    -inf], device='cuda:0')\n",
      "top3 indices: tensor([2511,   29,   13])\n",
      "outputs.logits: tensor([26.0000, 17.4219, 17.0000])\n",
      "outputs.scores: tensor([260000.,    -inf,    -inf], device='cuda:0')\n",
      "top3 indices: tensor([717, 220,   3])\n",
      "outputs.logits: tensor([25.4219, 14.8203, 13.4375])\n",
      "outputs.scores: tensor([254218.7500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([627, 198,  13])\n",
      "outputs.logits: tensor([22.2031, 21.1406, 16.3125])\n",
      "outputs.scores: tensor([222031.2500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([  791,  4516, 55915])\n",
      "outputs.logits: tensor([19.9531, 19.3906, 19.1250])\n",
      "outputs.scores: tensor([199531.2500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([ 2860,  2853, 29247])\n",
      "outputs.logits: tensor([22.4062, 22.0625, 20.6406])\n",
      "outputs.scores: tensor([224062.5000,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([2853, 3392, 3430])\n",
      "outputs.logits: tensor([21.3750, 19.8594, 19.4531])\n",
      "outputs.scores: tensor([213750.,    -inf,    -inf], device='cuda:0')\n",
      "top3 indices: tensor([315, 369, 374])\n",
      "outputs.logits: tensor([22.7344, 22.4219, 21.3281])\n",
      "outputs.scores: tensor([227343.7500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([  279,   220, 29247])\n",
      "outputs.logits: tensor([22.7500, 22.0938, 21.5469])\n",
      "outputs.scores: tensor([227500.,    -inf,    -inf], device='cuda:0')\n",
      "top3 indices: tensor([29247,   220,  3116])\n",
      "outputs.logits: tensor([23.3750, 21.4219, 18.6406])\n",
      "outputs.scores: tensor([233750.,    -inf,    -inf], device='cuda:0')\n",
      "top3 indices: tensor([374, 735, 690])\n",
      "outputs.logits: tensor([20.6719, 19.5938, 19.2344])\n",
      "outputs.scores: tensor([206718.7500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([ 400,  220, 9093])\n",
      "outputs.logits: tensor([23.6094, 21.5000, 18.6094])\n",
      "outputs.scores: tensor([236093.7500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([1272,  717,  220])\n",
      "outputs.logits: tensor([25.0625, 22.5000, 17.8750])\n",
      "outputs.scores: tensor([250625.,    -inf,    -inf], device='cuda:0')\n",
      "top3 indices: tensor([  489, 32819, 22104])\n",
      "outputs.logits: tensor([23.6562, 21.6094, 18.7344])\n",
      "outputs.scores: tensor([236562.5000,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([400,   3, 220])\n",
      "outputs.logits: tensor([25.3750, 21.0312, 20.0000])\n",
      "outputs.scores: tensor([253750.,    -inf,    -inf], device='cuda:0')\n",
      "top3 indices: tensor([ 717, 2501,  220])\n",
      "outputs.logits: tensor([25.9531, 18.3750, 18.1719])\n",
      "outputs.scores: tensor([259531.2500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([  284, 47481,    28])\n",
      "outputs.logits: tensor([24.4531, 21.1094, 19.8125])\n",
      "outputs.scores: tensor([244531.2500,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([ 400, 1134,  220])\n",
      "outputs.logits: tensor([24.4688, 18.7656, 18.0312])\n",
      "outputs.scores: tensor([244687.5000,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([2501, 4103,   27])\n",
      "outputs.logits: tensor([24.3125, 22.8125, 18.1250])\n",
      "outputs.scores: tensor([243125.,    -inf,    -inf], device='cuda:0')\n",
      "top3 indices: tensor([1272, 4103, 2983])\n",
      "outputs.logits: tensor([26.6562, 18.7500, 17.0781])\n",
      "outputs.scores: tensor([266562.5000,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([ 10,  28, 489])\n",
      "outputs.logits: tensor([33.3438, 22.9531, 20.5625])\n",
      "outputs.scores: tensor([333437.5000,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([  717, 30335, 87915])\n",
      "outputs.logits: tensor([34.3438, 16.7656, 15.7109])\n",
      "outputs.scores: tensor([343437.5000,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([ 28,  10, 284])\n",
      "outputs.logits: tensor([31.3750, 17.0781, 15.5547])\n",
      "outputs.scores: tensor([313750.,    -inf,    -inf], device='cuda:0')\n",
      "top3 indices: tensor([ 4103, 24130,  9756])\n",
      "outputs.logits: tensor([35.2500, 17.8594, 15.8672])\n",
      "outputs.scores: tensor([352500.,    -inf,    -inf], device='cuda:0')\n",
      "top3 indices: tensor([ 2511,  3662, 40171])\n",
      "outputs.logits: tensor([27.8750, 17.1406, 16.5156])\n",
      "outputs.scores: tensor([278750.,    -inf,    -inf], device='cuda:0')\n",
      "top3 indices: tensor([4103,  627,   13])\n",
      "outputs.logits: tensor([23.5000, 13.7578, 12.5781])\n",
      "outputs.scores: tensor([235000.,    -inf,    -inf], device='cuda:0')\n",
      "top3 indices: tensor([627, 198,  13])\n",
      "outputs.logits: tensor([22.6562, 21.3594, 17.4531])\n",
      "outputs.scores: tensor([226562.5000,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([  827,    42, 55915])\n",
      "outputs.logits: tensor([19.6250, 17.8906, 17.7188])\n",
      "outputs.scores: tensor([196250.,    -inf,    -inf], device='cuda:0')\n",
      "top3 indices: tensor([ 220,  400, 4103])\n",
      "outputs.logits: tensor([23.7812, 14.2578, 13.9297])\n",
      "outputs.scores: tensor([237812.5000,        -inf,        -inf], device='cuda:0')\n",
      "top3 indices: tensor([4103,  220,  717])\n",
      "outputs.logits: tensor([24.2500, 14.8359, 14.3516])\n",
      "outputs.scores: tensor([242500.,    -inf,    -inf], device='cuda:0')\n",
      "top3 indices: tensor([   271,   4815, 128009])\n",
      "outputs.logits: tensor([21.5938, 17.6406, 15.7344])\n",
      "outputs.scores: tensor([215937.5000,        -inf,        -inf], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'16 glasses is 8 pairs of glasses. 8 glasses cost 8 * $5 = $<<8*5=40>>40.\\n8 glasses are 4 glasses that cost 60% of the price. 4 glasses cost 4 * 0.6 * $5 = $<<4*0.6*5=12>>12.\\nThe total cost of the glasses is $40 + $12 = $<<40+12=52>>52.\\n#### 52'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_answer(input_text):\n",
    "    # Encode input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(model.device)\n",
    "    \n",
    "    # Generate output without hidden states for now\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_length=input_ids.shape[1] + 512,\n",
    "            # do_sample=True, top_k=1,\n",
    "            temperature=1e-4,\n",
    "            eos_token_id=tokenizer.encode(text='\\n\\n', add_special_tokens=False)[0],\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            return_dict_in_generate=True, \n",
    "            output_logits=True, \n",
    "            output_scores=True, # Probability scores\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "\n",
    "    print('outputs.logits[0].shape:', outputs.logits[0].shape) # outputs.logits: (seq_length, batch_size, vocab_size)\n",
    "\n",
    "    # Decode generated text\n",
    "    output_text = tokenizer.decode(outputs.sequences[0])\n",
    "    generated_answer = output_text.split('Answer: ')[-1].split('\\n\\n')[0]\n",
    "    generated_len = len(outputs.logits)\n",
    "\n",
    "    print('generated_len = output length - prompt length =', generated_len)\n",
    "\n",
    "    k = 3\n",
    "    topk_indices = torch.zeros((generated_len, k), dtype=torch.long)\n",
    "    topk_logits = torch.zeros((generated_len, k))\n",
    "    topk_prob = torch.zeros((generated_len, k))\n",
    "\n",
    "    # Iterate over each sequence position to find the top-3 indices and their probabilities\n",
    "    for seq_idx, logits_tensor in enumerate(outputs.logits):\n",
    "        # Calculate probabilities using softmax along the vocab dimension\n",
    "        logits = logits_tensor[0]  # score_tensor.shape: (batch_size, vocab_size)\n",
    "        \n",
    "        top_logit_values, top_logit_indices = torch.topk(logits, k=3)\n",
    "        \n",
    "        topk_indices[seq_idx] = top_logit_indices  # Indices of the top-3 tokens\n",
    "        topk_logits[seq_idx] = top_logit_values  # Logits of the top-3 tokens\n",
    "\n",
    "        print('top3 indices:', topk_indices[seq_idx])\n",
    "        print('outputs.logits:', topk_logits[seq_idx])\n",
    "        print('outputs.scores:', outputs.scores[seq_idx][0][topk_indices[seq_idx]])\n",
    "    return generated_answer\n",
    "\n",
    "generate_answer(question_to_prompt(test_qnas[sample_i]['question']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
